{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQOpar62cMMJ"
      },
      "source": [
        "# Terminology project \n",
        "\n",
        "### AGUIAR Mathilde NIAOURI Dimitra\n",
        "\n",
        "#### M2 NLP"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "DLW1ZVblu5ZH"
      },
      "source": [
        "# Goal and tools \n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "faBNXemu5Hv7"
      },
      "source": [
        "In this notebook we are using BERT model (bert-base-uncased) to predict IOB tagging on the NLP domain. \n",
        "To do so we use the BERT hosted on HuggingFace."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MHFsa7cpvWNL",
        "outputId": "f080315b-3cd1-4dae-c086-7989d10553ed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UWa0GcjdvK9E"
      },
      "source": [
        "#### Imports and packages "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tw1fGTwjxJXo",
        "outputId": "72391a1e-f3c3-4e57-a430-5c6b95b67710"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.8/dist-packages (4.25.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.8.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.11.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.4.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.9.24)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n"
          ]
        }
      ],
      "source": [
        "!pip3 install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6H8kq9QaLlig",
        "outputId": "5ca59d83-e4df-4193-eb9c-5108822278b8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: seqeval in /usr/local/lib/python3.8/dist-packages (1.2.2)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.8/dist-packages (from seqeval) (1.0.2)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.8/dist-packages (from seqeval) (1.21.6)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.7.3)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.21.3->seqeval) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.2.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install seqeval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "lBshl7bIxWA5"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import BertTokenizer, BertConfig, BertForTokenClassification\n",
        "import pandas as pd\n",
        "from pandas import DataFrame\n",
        "import glob\n",
        "from sklearn.metrics import accuracy_score\n",
        "import torch.nn as nn"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "_6DBeG6gQq1X"
      },
      "source": [
        "To use your GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WFpNeDHjxboz",
        "outputId": "0e8d65d5-3d5b-4006-c683-69c78d164cdc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cpu\n"
          ]
        }
      ],
      "source": [
        "from torch import cuda\n",
        "device = 'cuda' if cuda.is_available() else 'cpu'\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "npR8AO1T-i34"
      },
      "source": [
        "## Data preprocessing \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z1968-tsOy_F"
      },
      "source": [
        "### Important issue to address:\n",
        "\n",
        "BERT tokenize sub-words and not words. That's why we need strategies to deal with annotation on subwords instead of words. "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ⚠ Change the paths to yours "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-wOCXnDuzXzN"
      },
      "source": [
        "### Create all the dataframes of train, test and dev data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "QYX8mkQVzlC5"
      },
      "outputs": [],
      "source": [
        "def data_in_df(path):\n",
        "  all_files = glob.glob(path + \"/*.iob\")\n",
        "  l = []\n",
        "  cols = ['Token', 'Tag']\n",
        "\n",
        "  for filename in all_files:\n",
        "      df = pd.read_csv(filename, names = cols,on_bad_lines='skip', sep='\\s+', engine='python')\n",
        "      l.append(df)\n",
        "\n",
        "  df = pd.concat(l, axis=0, ignore_index=True)\n",
        "  return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "9q2TmS0hz7bW"
      },
      "outputs": [],
      "source": [
        "# Train df \n",
        "df_train = data_in_df('/content/drive/MyDrive/Terminology_project/curated/training_data/iob')\n",
        "# Dev df \n",
        "df_dev = data_in_df('/content/drive/MyDrive/Terminology_project/curated/dev_data/iob')\n",
        "# Test df \n",
        "df_test = data_in_df('/content/drive/MyDrive/Terminology_project/curated/test_data/iob_md')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JtSzlQnl0GnF",
        "outputId": "13fc1702-0f6c-4406-885d-b1c4c3cedb0d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of tokens in the dataset 19356\n",
            "Number of tokens in the dataset 2222\n",
            "Number of tokens in the dataset 2772\n"
          ]
        }
      ],
      "source": [
        "print(\"Number of tokens in the dataset\", len(df_train['Token']))\n",
        "print(\"Number of tokens in the dataset\",len(df_dev['Token']))\n",
        "print(\"Number of tokens in the dataset\",len(df_test['Token']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "7Mzef0_4I4uJ"
      },
      "outputs": [],
      "source": [
        "# Clean the tag columns \n",
        "def clean_tags(df):\n",
        "  # clean the tag column\n",
        "  print(df['Tag'])\n",
        "  df['Tag'] = df['Tag'].replace(['        I','    I','II'], 'I')\n",
        "  df['Tag'] = df['Tag'].replace(['        O','O ','0','o'], 'O')\n",
        "  df['Tag'] = df['Tag'].replace(['   B','b'], 'B')\n",
        "  return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C6YWle8jJGOs",
        "outputId": "7f5b2795-8e30-4d2f-be4d-808fe6ee7dce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0            O\n",
            "1            O\n",
            "2            O\n",
            "3            O\n",
            "4            O\n",
            "         ...  \n",
            "19351    B-NLP\n",
            "19352    I-NLP\n",
            "19353    I-NLP\n",
            "19354    I-NLP\n",
            "19355        O\n",
            "Name: Tag, Length: 19356, dtype: object\n",
            "0       O\n",
            "1       O\n",
            "2       O\n",
            "3       O\n",
            "4       O\n",
            "       ..\n",
            "2217    O\n",
            "2218    O\n",
            "2219    O\n",
            "2220    O\n",
            "2221    O\n",
            "Name: Tag, Length: 2222, dtype: object\n",
            "0           O\n",
            "1           O\n",
            "2           O\n",
            "3       B-NLP\n",
            "4       I-NLP\n",
            "        ...  \n",
            "2767        O\n",
            "2768        O\n",
            "2769    B-NLP\n",
            "2770        O\n",
            "2771        O\n",
            "Name: Tag, Length: 2772, dtype: object\n"
          ]
        }
      ],
      "source": [
        "df_train = clean_tags(df_train)\n",
        "# Dev df \n",
        "df_dev = clean_tags(df_dev)\n",
        "# Test df \n",
        "df_test = clean_tags(df_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N5AgPi_1zfZL"
      },
      "source": [
        "### Reconstruct sentences and sequence of tags"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "rr5EoQRixtaJ"
      },
      "outputs": [],
      "source": [
        "def reconstruct_sent(df):\n",
        "    sentences = []\n",
        "    tags = []\n",
        "    tmp_words = []\n",
        "    tmp_tag = []\n",
        "    idx = 0\n",
        "    for i in df['Token'].values:\n",
        "        if i!= \".\":\n",
        "            tmp_words.append(str(i)+\" \")\n",
        "            tag = df['Tag'].iloc[idx]\n",
        "            tmp_tag.append(str(tag))\n",
        "        else:\n",
        "            tmp_words.append(str(i))\n",
        "            sentence = ''.join(tmp_words)\n",
        "            sentences.append(sentence)\n",
        "            tmp_words = []\n",
        "            tmp_tag.append(\"O\")\n",
        "            tags_seq = ','.join(tmp_tag)\n",
        "            tags.append(tags_seq)\n",
        "            tmp_tag = []\n",
        "        idx+=1\n",
        "\n",
        "\n",
        "    df = pd.DataFrame(list(zip(sentences, tags)), columns =['Sentence', 'Tags'])\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "7aVWLC1h0aaV"
      },
      "outputs": [],
      "source": [
        "# Train df with sentences instead of tokens\n",
        "df_train_sents = reconstruct_sent(df_train)\n",
        "# Dev df with sentences instead of tokens\n",
        "df_dev_sents = reconstruct_sent(df_dev)\n",
        "# Test df with sentences instead of tokens\n",
        "df_test_sents = reconstruct_sent(df_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mItXFndz3FXY",
        "outputId": "a693cab0-5e07-4c59-f966-54f85bce57c8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of sentences in the dataset 692\n",
            "Number of tokens in the dataset 82\n",
            "Number of tokens in the dataset 105\n"
          ]
        }
      ],
      "source": [
        "print(\"Number of sentences in the dataset\", len(df_train_sents['Sentence']))\n",
        "print(\"Number of tokens in the dataset\",len(df_dev_sents['Sentence']))\n",
        "print(\"Number of tokens in the dataset\",len(df_test_sents['Sentence']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "arGLyofo81hJ",
        "outputId": "5a675196-0fe9-4d9c-dad0-b740145d3681"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'B-NLP': 1, 'I-NLP': 2, 'O': 0}"
            ]
          },
          "execution_count": 65,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "label2id = {'B-NLP':1, 'I-NLP':2, 'O':0}  #{k: v for v, k in enumerate(df_train.Tags.unique())}  , 'nan':3\n",
        "id2label = {1:'B-NLP', 2:'I-NLP', 0:'O'}  #{v: k for v, k in enumerate(df_train.Tags.unique())}  , 3:'nan'\n",
        "label2id"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HiGKi7yl1cyJ"
      },
      "source": [
        "## Tokenization\n",
        "\n",
        "We try to preserve the alignments with the labels and their subwords"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9V9Smpq42WRT"
      },
      "source": [
        "We use the BERT base uncased tokenizer "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "ypp0rnKG2VDh"
      },
      "outputs": [],
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "AJEMvkYv1mWa"
      },
      "outputs": [],
      "source": [
        "def tokenize_and_preserve_labels(sentence, text_labels, tokenizer):\n",
        "    \"\"\"\n",
        "    Word piece tokenization makes it difficult to match word labels\n",
        "    back up with individual word pieces. This function tokenizes each\n",
        "    word one at a time so that it is easier to preserve the correct\n",
        "    label for each subword. It is, of course, a bit slower in processing\n",
        "    time, but it will help our model achieve higher accuracy.\n",
        "    \"\"\"\n",
        "\n",
        "    tokenized_sentence = []\n",
        "    labels = []\n",
        "\n",
        "    sentence = sentence.strip()\n",
        "\n",
        "    for word, label in zip(sentence.split(), text_labels.split(\",\")):\n",
        "\n",
        "        # Tokenize the word and count # of subwords the word is broken into\n",
        "        tokenized_word = tokenizer.tokenize(word)\n",
        "        n_subwords = len(tokenized_word)\n",
        "\n",
        "        # Add the tokenized word to the final tokenized word list\n",
        "        tokenized_sentence.extend(tokenized_word)\n",
        "\n",
        "        # Add the same label to the new list of labels `n_subwords` times\n",
        "        labels.extend([label] * n_subwords)\n",
        "\n",
        "    return tokenized_sentence, labels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ByzwakjW1sA_",
        "outputId": "c3b63227-f5b6-49e7-daf5-07546767f1a6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(['furthermore',\n",
              "  ',',\n",
              "  'the',\n",
              "  'empirical',\n",
              "  'formula',\n",
              "  '##e',\n",
              "  'based',\n",
              "  'on',\n",
              "  'the',\n",
              "  'results',\n",
              "  'can',\n",
              "  'be',\n",
              "  'used',\n",
              "  'to',\n",
              "  'predict',\n",
              "  'the',\n",
              "  'parameter',\n",
              "  'in',\n",
              "  'esa',\n",
              "  'to',\n",
              "  'avoid',\n",
              "  'parameter',\n",
              "  'estimation',\n",
              "  'that',\n",
              "  'is',\n",
              "  'usually',\n",
              "  'time',\n",
              "  '-',\n",
              "  'consuming',\n",
              "  '.'],\n",
              " ['O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'B-NLP',\n",
              "  'O',\n",
              "  'B-NLP',\n",
              "  'O',\n",
              "  'O',\n",
              "  'B-NLP',\n",
              "  'I-NLP',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O'])"
            ]
          },
          "execution_count": 69,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "### This cell is for info purpose only ###\n",
        "# For the train data\n",
        "tokenize_and_preserve_labels(df_train_sents['Sentence'].iloc[20], df_train_sents['Tags'].iloc[20], tokenizer)\n",
        "# For the dev data\n",
        "tokenize_and_preserve_labels(df_dev_sents['Sentence'].iloc[20], df_dev_sents['Tags'].iloc[20], tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GuRT_jtq3F-7"
      },
      "source": [
        "Quick demo of a tokenized example from our dataframe "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "snV-DgB728Va"
      },
      "source": [
        "# Pytorch Datsets and Dataloaders preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k2YxlpPq328f"
      },
      "source": [
        "## Pytorch Dataset classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "wTS-upJ53Arf"
      },
      "outputs": [],
      "source": [
        "class dataset(Dataset):\n",
        "    def __init__(self, dataframe, tokenizer, max_len):\n",
        "        self.len = len(dataframe)\n",
        "        self.data = dataframe\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "        \n",
        "    def __getitem__(self, index):\n",
        "        # step 1: tokenize (and adapt corresponding labels)\n",
        "        sentence = self.data.Sentence[index]  \n",
        "        word_labels = self.data.Tags[index]  \n",
        "        tokenized_sentence, labels = tokenize_and_preserve_labels(sentence, word_labels, self.tokenizer)\n",
        "        \n",
        "        # step 2: add special tokens (and corresponding labels)\n",
        "        tokenized_sentence = [\"[CLS]\"] + tokenized_sentence + [\"[SEP]\"] # add special tokens\n",
        "        labels.insert(0, \"O\") # add outside label for [CLS] token\n",
        "        labels.insert(-1, \"O\") # add outside label for [SEP] token\n",
        "\n",
        "        # step 3: truncating/padding\n",
        "        maxlen = self.max_len\n",
        "\n",
        "        if (len(tokenized_sentence) > maxlen):\n",
        "          # truncate\n",
        "          tokenized_sentence = tokenized_sentence[:maxlen]\n",
        "          labels = labels[:maxlen]\n",
        "        else:\n",
        "          # pad\n",
        "          tokenized_sentence = tokenized_sentence + ['[PAD]'for _ in range(maxlen - len(tokenized_sentence))]\n",
        "          labels = labels + [\"O\" for _ in range(maxlen - len(labels))]\n",
        "\n",
        "        # step 4: obtain the attention mask\n",
        "        attn_mask = [1 if tok != '[PAD]' else 0 for tok in tokenized_sentence]\n",
        "        \n",
        "        # step 5: convert tokens to input ids\n",
        "        ids = self.tokenizer.convert_tokens_to_ids(tokenized_sentence)\n",
        "\n",
        "        label_ids = [label2id[label] for label in labels]\n",
        "        \n",
        "        return {\n",
        "              'ids': torch.tensor(ids, dtype=torch.long),\n",
        "              'mask': torch.tensor(attn_mask, dtype=torch.long),\n",
        "              'targets': torch.tensor(label_ids, dtype=torch.long)\n",
        "        } \n",
        "    \n",
        "    def __len__(self):\n",
        "        return self.len"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8DlJH9gb4C3c"
      },
      "source": [
        "### Hyperparameters that help define the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "UtQlEAlN4arH"
      },
      "outputs": [],
      "source": [
        "MAX_LEN = 64 \n",
        "TRAIN_BATCH_SIZE = 4\n",
        "VALID_BATCH_SIZE = 2\n",
        "EPOCHS = 1 \n",
        "LEARNING_RATE = 1e-05  \n",
        "MAX_GRAD_NORM = 10 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "zrG74VfF4bts"
      },
      "outputs": [],
      "source": [
        "training_set = dataset(df_train_sents, tokenizer, MAX_LEN)\n",
        "dev_set = dataset(df_dev_sents, tokenizer, MAX_LEN)\n",
        "testing_set = dataset(df_test_sents, tokenizer, MAX_LEN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cIUgbvvQVXJw",
        "outputId": "c995faff-7532-4c5e-dc6c-766fbbc8c877"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'ids': tensor([  101,  1999,  2023,  3720,  2057,  6848,  2195, 12046,  2015,  1997,\n",
              "          2522,  5886, 10127,  4225,  2478,  2415,  2075,  3399,  1998,  8556,\n",
              "          1996,  6179,  2791,  1997,  2107, 12046,  2015,  2005,  2592, 13063,\n",
              "          1999,  6882,  3793,  4245,  1012,   102,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0]),\n",
              " 'mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " 'targets': tensor([0, 0, 0, 0, 0, 0, 0, 1, 1, 2, 2, 2, 2, 0, 0, 1, 1, 2, 0, 0, 0, 0, 0, 0,\n",
              "         0, 1, 1, 0, 0, 0, 0, 1, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}"
            ]
          },
          "execution_count": 73,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "training_set[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cXTg-EqdUDKF",
        "outputId": "83c34b01-3856-489a-d784-bc0bc63a78c0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'ids': tensor([  101,  1996,  3463,  1997,  5164, 16293,  2015,  2006,  5046,  3128,\n",
              "          2653,  8720,  2024,  2036,  6022,  2488,  2084,  1996,  2110,  1011,\n",
              "          1997,  1011,  1996,  1011,  2396,  3921,  1012,   102,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0]),\n",
              " 'mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " 'targets': tensor([0, 0, 0, 0, 1, 2, 2, 0, 0, 1, 2, 2, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2,\n",
              "         2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}"
            ]
          },
          "execution_count": 74,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "testing_set[26]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yF64g0M3VrN9",
        "outputId": "36f0c4f2-0bcb-4cc4-c7a9-4d88bbe29b75"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CLS]       O\n",
            "in          O\n",
            "this        O\n",
            "article     O\n",
            "we          O\n",
            "discuss     O\n",
            "several     O\n",
            "metric      B-NLP\n",
            "##s         B-NLP\n",
            "of          I-NLP\n",
            "co          I-NLP\n",
            "##her       I-NLP\n",
            "##ence      I-NLP\n",
            "defined     O\n",
            "using       O\n",
            "center      B-NLP\n",
            "##ing       B-NLP\n",
            "theory      I-NLP\n",
            "and         O\n",
            "investigate  O\n",
            "the         O\n",
            "useful      O\n",
            "##ness      O\n",
            "of          O\n",
            "such        O\n",
            "metric      B-NLP\n",
            "##s         B-NLP\n",
            "for         O\n",
            "information  O\n",
            "ordering    O\n"
          ]
        }
      ],
      "source": [
        "# print the first 30 tokens and corresponding labels\n",
        "for token, label in zip(tokenizer.convert_ids_to_tokens(training_set[0][\"ids\"][:30]), training_set[0][\"targets\"][:30]):\n",
        "  print('{0:10}  {1}'.format(token, id2label[label.item()]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XOB926Z939S7"
      },
      "source": [
        "## Pytorch Dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "8vvJmC-h43nB"
      },
      "outputs": [],
      "source": [
        "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
        "                'shuffle': True,\n",
        "                'num_workers': 0\n",
        "                }\n",
        "\n",
        "test_params = {'batch_size': VALID_BATCH_SIZE,\n",
        "                'shuffle': True,\n",
        "                'num_workers': 0\n",
        "                }\n",
        "\n",
        "training_loader = DataLoader(training_set, **train_params)\n",
        "testing_loader = DataLoader(testing_set, **test_params)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZeDVSRL6JVU"
      },
      "source": [
        "# Model definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "opT693iP6Pe3",
        "outputId": "771a7bea-8db6-466f-b11f-13abcf1dfc84"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForTokenClassification: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']\n",
            "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "BertForTokenClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 78,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = BertForTokenClassification.from_pretrained('bert-base-uncased',  \n",
        "                                                   num_labels=len(id2label),\n",
        "                                                   id2label=id2label,\n",
        "                                                   label2id=label2id)\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Gan4WNL-EdG",
        "outputId": "f7d87c77-f603-4658-9ce6-7019f1e64b60"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(1.0953, grad_fn=<NllLossBackward0>)"
            ]
          },
          "execution_count": 79,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ids = training_set[0][\"ids\"].unsqueeze(0)\n",
        "mask = training_set[0][\"mask\"].unsqueeze(0)\n",
        "targets = training_set[0][\"targets\"].unsqueeze(0)\n",
        "ids = ids.to(device)\n",
        "mask = mask.to(device)\n",
        "targets = targets.to(device)\n",
        "outputs = model(input_ids=ids, attention_mask=mask, labels=targets)\n",
        "initial_loss = outputs[0]\n",
        "initial_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UPv8Wz23-Wjj",
        "outputId": "6a2845b6-1534-4557-b306-4360dd4ab0cc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([1, 64, 3])"
            ]
          },
          "execution_count": 80,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tr_logits = outputs[1]\n",
        "tr_logits.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "VgYzRsz0-aZz"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.Adam(params=model.parameters(), lr=LEARNING_RATE)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Training "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-ESdj366ac9"
      },
      "source": [
        "## Training function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "tdMz5K3S-gLF"
      },
      "outputs": [],
      "source": [
        "# Defining the training function on the 80% of the dataset for tuning the bert model\n",
        "def train(epoch):\n",
        "    tr_loss, tr_accuracy = 0, 0\n",
        "    nb_tr_examples, nb_tr_steps = 0, 0\n",
        "    tr_preds, tr_labels = [], []\n",
        "    # put model in training mode\n",
        "    model.train()\n",
        "    \n",
        "    for idx, batch in enumerate(training_loader):\n",
        "        \n",
        "        ids = batch['ids'].to(device, dtype = torch.long)\n",
        "        mask = batch['mask'].to(device, dtype = torch.long)\n",
        "        targets = batch['targets'].to(device, dtype = torch.long)\n",
        "\n",
        "        outputs = model(input_ids=ids, attention_mask=mask, labels=targets)\n",
        "        loss, tr_logits = outputs.loss, outputs.logits\n",
        "        tr_loss += loss.item()\n",
        "\n",
        "        nb_tr_steps += 1\n",
        "        nb_tr_examples += targets.size(0)\n",
        "        \n",
        "        if idx % 100==0:\n",
        "            loss_step = tr_loss/nb_tr_steps\n",
        "            print(f\"Training loss per 100 training steps: {loss_step}\")\n",
        "           \n",
        "        # compute training accuracy\n",
        "        flattened_targets = targets.view(-1) # shape (batch_size * seq_len,)\n",
        "        active_logits = tr_logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n",
        "        flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size * seq_len,)\n",
        "        # now, use mask to determine where we should compare predictions with targets (includes [CLS] and [SEP] token predictions)\n",
        "        active_accuracy = mask.view(-1) == 1 # active accuracy is also of shape (batch_size * seq_len,)\n",
        "        targets = torch.masked_select(flattened_targets, active_accuracy)\n",
        "        predictions = torch.masked_select(flattened_predictions, active_accuracy)\n",
        "        \n",
        "        tr_preds.extend(predictions)\n",
        "        tr_labels.extend(targets)\n",
        "        \n",
        "        tmp_tr_accuracy = accuracy_score(targets.cpu().numpy(), predictions.cpu().numpy())\n",
        "        tr_accuracy += tmp_tr_accuracy\n",
        "    \n",
        "        # gradient clipping\n",
        "        torch.nn.utils.clip_grad_norm_(\n",
        "            parameters=model.parameters(), max_norm=MAX_GRAD_NORM\n",
        "        )\n",
        "        \n",
        "        # backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    epoch_loss = tr_loss / nb_tr_steps\n",
        "    tr_accuracy = tr_accuracy / nb_tr_steps\n",
        "    print(f\"Training loss epoch: {epoch_loss}\")\n",
        "    print(f\"Training accuracy epoch: {tr_accuracy}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bANQt9pT-nFC",
        "outputId": "074d796d-cd06-4d39-c97f-cdfac5bb6fb3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training epoch: 1\n",
            "Training loss per 100 training steps: 1.1123508214950562\n",
            "Training loss per 100 training steps: 0.3489703205552432\n",
            "Training loss epoch: 0.2839571420598581\n",
            "Training accuracy epoch: 0.8072301541917124\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(EPOCHS):\n",
        "    print(f\"Training epoch: {epoch + 1}\")\n",
        "    train(epoch)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "_IEO5yjE6cN3"
      },
      "source": [
        "# Model Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jmdzpr626gYv"
      },
      "source": [
        "## Eval function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "7ofTCHtW-uK-"
      },
      "outputs": [],
      "source": [
        "def valid(model, testing_loader):\n",
        "    # put model in evaluation mode\n",
        "    model.eval()\n",
        "    \n",
        "    eval_loss, eval_accuracy = 0, 0\n",
        "    nb_eval_examples, nb_eval_steps = 0, 0\n",
        "    eval_preds, eval_labels = [], []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for idx, batch in enumerate(testing_loader):\n",
        "            \n",
        "            ids = batch['ids'].to(device, dtype = torch.long)\n",
        "            mask = batch['mask'].to(device, dtype = torch.long)\n",
        "            targets = batch['targets'].to(device, dtype = torch.long)\n",
        "            \n",
        "            outputs = model(input_ids=ids, attention_mask=mask, labels=targets)\n",
        "            loss, eval_logits = outputs.loss, outputs.logits\n",
        "            \n",
        "            eval_loss += loss.item()\n",
        "\n",
        "            nb_eval_steps += 1\n",
        "            nb_eval_examples += targets.size(0)\n",
        "        \n",
        "            if idx % 100==0:\n",
        "                loss_step = eval_loss/nb_eval_steps\n",
        "                print(f\"Validation loss per 100 evaluation steps: {loss_step}\")\n",
        "              \n",
        "            # compute evaluation accuracy\n",
        "            flattened_targets = targets.view(-1) # shape (batch_size * seq_len,)\n",
        "            active_logits = eval_logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n",
        "            flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size * seq_len,)\n",
        "            # now, use mask to determine where we should compare predictions with targets (includes [CLS] and [SEP] token predictions)\n",
        "            active_accuracy = mask.view(-1) == 1 # active accuracy is also of shape (batch_size * seq_len,)\n",
        "            targets = torch.masked_select(flattened_targets, active_accuracy)\n",
        "            predictions = torch.masked_select(flattened_predictions, active_accuracy)\n",
        "            \n",
        "            eval_labels.extend(targets)\n",
        "            eval_preds.extend(predictions)\n",
        "            \n",
        "            tmp_eval_accuracy = accuracy_score(targets.cpu().numpy(), predictions.cpu().numpy())\n",
        "            eval_accuracy += tmp_eval_accuracy\n",
        "    \n",
        "    print(eval_labels)\n",
        "    print(eval_preds)\n",
        "\n",
        "    labels = [id2label[id.item()] for id in eval_labels]\n",
        "    predictions = [id2label[id.item()] for id in eval_preds]\n",
        "\n",
        "    print(labels)\n",
        "    print(predictions)\n",
        "    \n",
        "    eval_loss = eval_loss / nb_eval_steps\n",
        "    eval_accuracy = eval_accuracy / nb_eval_steps\n",
        "    print(f\"Validation Loss: {eval_loss}\")\n",
        "    print(f\"Validation Accuracy: {eval_accuracy}\")\n",
        "\n",
        "    return labels, predictions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KMa07bpT-z5g",
        "outputId": "a273dfd9-4fb5-417f-a35e-f88c9f2fab22"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation loss per 100 evaluation steps: 0.2417120337486267\n",
            "[tensor(0), tensor(1), tensor(1), tensor(1), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(2), tensor(2), tensor(2), tensor(2), tensor(2), tensor(2), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(2), tensor(2), tensor(2), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(2), tensor(2), tensor(2), tensor(0), tensor(1), tensor(1), tensor(2), tensor(2), tensor(2), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(2), tensor(2), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(2), tensor(2), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(2), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(2), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(0), tensor(1), tensor(1), tensor(2), tensor(0), tensor(0), tensor(1), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(2), tensor(2), tensor(2), tensor(2), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(2), tensor(0), tensor(1), tensor(2), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(2), tensor(2), tensor(2), tensor(2), tensor(0), tensor(1), tensor(1), tensor(1), tensor(2), tensor(2), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(2), tensor(2), tensor(0), tensor(1), tensor(1), tensor(2), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(2), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(2), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(2), tensor(2), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(2), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(2), tensor(0), tensor(1), tensor(1), tensor(2), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(2), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(2), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(2), tensor(2), tensor(2), tensor(2), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(2), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(2), tensor(1), tensor(2), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(2), tensor(2), tensor(2), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(2), tensor(0), tensor(0), tensor(1), tensor(2), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(2), tensor(2), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(2), tensor(2), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(2), tensor(2), tensor(2), tensor(2), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(2), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(2), tensor(2), tensor(0), tensor(0), tensor(1), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(2), tensor(2), tensor(2), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(2), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(2), tensor(0), tensor(1), tensor(0), tensor(1), tensor(2), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(0), tensor(1), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(2), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(2), tensor(2), tensor(2), tensor(2), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(2), tensor(0), tensor(0), tensor(1), tensor(2), tensor(2), tensor(2), tensor(2), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(2), tensor(2), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(2), tensor(0), tensor(0), tensor(1), tensor(2), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(2), tensor(2), tensor(2), tensor(2), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(2), tensor(2), tensor(2), tensor(2), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(2), tensor(2), tensor(2), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(2), tensor(2), tensor(2), tensor(2), tensor(2), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(0), tensor(0), tensor(1), tensor(2), tensor(2), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(2), tensor(0), tensor(0), tensor(1), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(0), tensor(1), tensor(2), tensor(2), tensor(2), tensor(2), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(2), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(2), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(2), tensor(2), tensor(2), tensor(2), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(2), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(2), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(2), tensor(2), tensor(2), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(2), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(2), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(0), tensor(1), tensor(1), tensor(2), tensor(2), tensor(2), tensor(2), tensor(0), tensor(1), tensor(1), tensor(2), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(2), tensor(2), tensor(2), tensor(0), tensor(1), tensor(2), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(2), tensor(2), tensor(2), tensor(0), tensor(0), tensor(1), tensor(2), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(2), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(2), tensor(2), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(2), tensor(0), tensor(0), tensor(1), tensor(2), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(2), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(2), tensor(2), tensor(2), tensor(2), tensor(2), tensor(1), tensor(2), tensor(2), tensor(2), tensor(0), tensor(0), tensor(1), tensor(1), tensor(2), tensor(2), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(2), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(2), tensor(2), tensor(2), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(2), tensor(2), tensor(2), tensor(2), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(0), tensor(1), tensor(2), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(2), tensor(2), tensor(0), tensor(1), tensor(0), tensor(2), tensor(1), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(0), tensor(1), tensor(2), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(2), tensor(2), tensor(0), tensor(2), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(2), tensor(2), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(2), tensor(2), tensor(0), tensor(0), tensor(1), tensor(2), tensor(2), tensor(2), tensor(2), tensor(2), tensor(2), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(2), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(2), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(0), tensor(0), tensor(1), tensor(2), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(2), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(2), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(2), tensor(0), tensor(1), tensor(0), tensor(1), tensor(2), tensor(2), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(2), tensor(0), tensor(1), tensor(1), tensor(2), tensor(0), tensor(0), tensor(1), tensor(2), tensor(2), tensor(2), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(2), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(2), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(0), tensor(1), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(2), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(2), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(2), tensor(2), tensor(0), tensor(1), tensor(1), tensor(2), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(2), tensor(2), tensor(2), tensor(2), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(2), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(2), tensor(2), tensor(2), tensor(2), tensor(2), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(2), tensor(2), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(0), tensor(0), tensor(1), tensor(2), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(2), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(2), tensor(1), tensor(0), tensor(1), tensor(2), tensor(2), tensor(0), tensor(0), tensor(1), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(2), tensor(2), tensor(2), tensor(2), tensor(2), tensor(2), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(2), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(2), tensor(2), tensor(2), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(2), tensor(0), tensor(0), tensor(1), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(0), tensor(1), tensor(1), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(2), tensor(0), tensor(0), tensor(1), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(2), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(0), tensor(0)]\n",
            "[tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(2), tensor(2), tensor(2), tensor(2), tensor(2), tensor(2), tensor(0), tensor(1), tensor(1), tensor(1), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(2), tensor(2), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(2), tensor(2), tensor(2), tensor(0), tensor(1), tensor(1), tensor(2), tensor(2), tensor(2), tensor(0), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(2), tensor(2), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(2), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(2), tensor(2), tensor(0), tensor(1), tensor(2), tensor(2), tensor(2), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(2), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(2), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(2), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(0), tensor(1), tensor(1), tensor(2), tensor(0), tensor(0), tensor(1), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(2), tensor(2), tensor(2), tensor(2), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(2), tensor(0), tensor(0), tensor(1), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(2), tensor(2), tensor(0), tensor(1), tensor(1), tensor(1), tensor(2), tensor(2), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(2), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(2), tensor(2), tensor(0), tensor(1), tensor(1), tensor(2), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(2), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(2), tensor(0), tensor(0), tensor(1), tensor(2), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(2), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(2), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(2), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(2), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(2), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(2), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(2), tensor(2), tensor(2), tensor(0), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(2), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(2), tensor(2), tensor(2), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(2), tensor(2), tensor(2), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(2), tensor(2), tensor(0), tensor(1), tensor(0), tensor(1), tensor(2), tensor(0), tensor(0), tensor(1), tensor(2), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(2), tensor(2), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(2), tensor(0), tensor(1), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(2), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(2), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(2), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(2), tensor(2), tensor(2), tensor(2), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(2), tensor(0), tensor(1), tensor(1), tensor(2), tensor(1), tensor(1), tensor(1), tensor(1), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(2), tensor(2), tensor(2), tensor(0), tensor(1), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(2), tensor(0), tensor(0), tensor(1), tensor(2), tensor(2), tensor(2), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(2), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(2), tensor(0), tensor(1), tensor(0), tensor(1), tensor(2), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(0), tensor(1), tensor(2), tensor(0), tensor(1), tensor(2), tensor(0), tensor(1), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(2), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(2), tensor(1), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(2), tensor(0), tensor(0), tensor(1), tensor(2), tensor(2), tensor(0), tensor(1), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(2), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(2), tensor(0), tensor(0), tensor(1), tensor(2), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(2), tensor(2), tensor(2), tensor(2), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(2), tensor(2), tensor(2), tensor(0), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(2), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(2), tensor(2), tensor(2), tensor(0), tensor(2), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(0), tensor(0), tensor(1), tensor(2), tensor(2), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(2), tensor(0), tensor(0), tensor(0), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(0), tensor(1), tensor(2), tensor(2), tensor(2), tensor(2), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(2), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(2), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(2), tensor(2), tensor(2), tensor(1), tensor(2), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(2), tensor(2), tensor(1), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(2), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(0), tensor(0), tensor(1), tensor(1), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(0), tensor(1), tensor(2), tensor(2), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(2), tensor(2), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(2), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(2), tensor(2), tensor(0), tensor(0), tensor(1), tensor(2), tensor(2), tensor(0), tensor(0), tensor(1), tensor(2), tensor(2), tensor(2), tensor(2), tensor(2), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(2), tensor(2), tensor(1), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(2), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(2), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(2), tensor(1), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(0), tensor(1), tensor(1), tensor(2), tensor(2), tensor(2), tensor(2), tensor(0), tensor(1), tensor(1), tensor(2), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(2), tensor(0), tensor(0), tensor(1), tensor(2), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(2), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(2), tensor(2), tensor(0), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(2), tensor(2), tensor(2), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(2), tensor(2), tensor(2), tensor(0), tensor(1), tensor(2), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(2), tensor(1), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(2), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(2), tensor(2), tensor(0), tensor(1), tensor(2), tensor(2), tensor(2), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(2), tensor(2), tensor(2), tensor(2), tensor(1), tensor(2), tensor(2), tensor(2), tensor(2), tensor(0), tensor(0), tensor(1), tensor(1), tensor(2), tensor(2), tensor(2), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(2), tensor(2), tensor(2), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(2), tensor(2), tensor(2), tensor(0), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(0), tensor(1), tensor(2), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(2), tensor(2), tensor(0), tensor(1), tensor(2), tensor(2), tensor(2), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(0), tensor(1), tensor(2), tensor(2), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(2), tensor(2), tensor(0), tensor(1), tensor(1), tensor(0), tensor(2), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(2), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(2), tensor(2), tensor(0), tensor(0), tensor(1), tensor(2), tensor(2), tensor(2), tensor(2), tensor(2), tensor(2), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(2), tensor(2), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(2), tensor(0), tensor(1), tensor(1), tensor(1), tensor(2), tensor(2), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(2), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(0), tensor(0), tensor(1), tensor(2), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(2), tensor(0), tensor(1), tensor(1), tensor(1), tensor(2), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(2), tensor(2), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(2), tensor(0), tensor(1), tensor(1), tensor(0), tensor(2), tensor(2), tensor(0), tensor(1), tensor(0), tensor(1), tensor(2), tensor(2), tensor(0), tensor(1), tensor(2), tensor(1), tensor(2), tensor(2), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(2), tensor(0), tensor(1), tensor(1), tensor(2), tensor(0), tensor(0), tensor(1), tensor(2), tensor(2), tensor(2), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(2), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(2), tensor(0), tensor(1), tensor(1), tensor(0), tensor(2), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(0), tensor(1), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(2), tensor(0), tensor(1), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(2), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(2), tensor(2), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(0), tensor(1), tensor(2), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(2), tensor(2), tensor(0), tensor(1), tensor(1), tensor(2), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(2), tensor(2), tensor(2), tensor(2), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(2), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(2), tensor(2), tensor(2), tensor(2), tensor(1), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(1), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(2), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(2), tensor(2), tensor(0), tensor(1), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(2), tensor(2), tensor(0), tensor(0), tensor(1), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(2), tensor(2), tensor(2), tensor(2), tensor(2), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(2), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(0), tensor(1), tensor(2), tensor(2), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(0), tensor(0), tensor(1), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(0), tensor(1), tensor(1), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(0), tensor(0), tensor(1), tensor(2), tensor(2), tensor(0), tensor(0), tensor(1), tensor(2), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(2), tensor(2), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(2), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(2), tensor(0), tensor(0)]\n",
            "['O', 'B-NLP', 'B-NLP', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'B-NLP', 'B-NLP', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'B-NLP', 'B-NLP', 'B-NLP', 'B-NLP', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'B-NLP', 'O', 'B-NLP', 'B-NLP', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'B-NLP', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'B-NLP', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'B-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'B-NLP', 'B-NLP', 'B-NLP', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'O', 'B-NLP', 'B-NLP', 'O', 'O', 'B-NLP', 'B-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'B-NLP', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'B-NLP', 'B-NLP', 'B-NLP', 'B-NLP', 'B-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'B-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'B-NLP', 'B-NLP', 'I-NLP', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'O', 'B-NLP', 'B-NLP', 'B-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'B-NLP', 'B-NLP', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'B-NLP', 'O', 'O', 'O', 'O', 'B-NLP', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'B-NLP', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'B-NLP', 'B-NLP', 'I-NLP', 'O', 'O', 'B-NLP', 'B-NLP', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'O', 'B-NLP', 'B-NLP', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'B-NLP', 'B-NLP', 'B-NLP', 'B-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'B-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'B-NLP', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'B-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'B-NLP', 'B-NLP', 'O', 'B-NLP', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'B-NLP', 'B-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'B-NLP', 'O', 'B-NLP', 'B-NLP', 'B-NLP', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'B-NLP', 'B-NLP', 'B-NLP', 'B-NLP', 'B-NLP', 'B-NLP', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'B-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'B-NLP', 'I-NLP', 'O', 'B-NLP', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'B-NLP', 'O', 'O', 'O', 'O', 'B-NLP', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'O', 'B-NLP', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'B-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'B-NLP', 'B-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'B-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'B-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'B-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'B-NLP', 'B-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'B-NLP', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'B-NLP', 'B-NLP', 'I-NLP', 'O', 'B-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'B-NLP', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'B-NLP', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'B-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'O', 'B-NLP', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'B-NLP', 'B-NLP', 'B-NLP', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'B-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'B-NLP', 'B-NLP', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'B-NLP', 'O', 'B-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'B-NLP', 'B-NLP', 'I-NLP', 'I-NLP', 'B-NLP', 'B-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'B-NLP', 'O', 'I-NLP', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'B-NLP', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'B-NLP', 'O', 'O', 'B-NLP', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'B-NLP', 'O', 'B-NLP', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'B-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'B-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'I-NLP', 'O', 'O', 'B-NLP', 'B-NLP', 'B-NLP', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'B-NLP', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'B-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'B-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'B-NLP', 'O', 'B-NLP', 'B-NLP', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'B-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'B-NLP', 'B-NLP', 'O', 'B-NLP', 'B-NLP', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'B-NLP', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'B-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'I-NLP', 'O', 'B-NLP', 'B-NLP', 'I-NLP', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'B-NLP', 'B-NLP', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'B-NLP', 'B-NLP', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'B-NLP', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'B-NLP', 'B-NLP', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'B-NLP', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'B-NLP', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'B-NLP', 'B-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'B-NLP', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'B-NLP', 'B-NLP', 'B-NLP', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'B-NLP', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'B-NLP', 'B-NLP', 'B-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O']\n",
            "['O', 'B-NLP', 'B-NLP', 'B-NLP', 'O', 'O', 'B-NLP', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'B-NLP', 'B-NLP', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'B-NLP', 'B-NLP', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'B-NLP', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'B-NLP', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'B-NLP', 'O', 'B-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'B-NLP', 'B-NLP', 'B-NLP', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'O', 'B-NLP', 'O', 'O', 'B-NLP', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'B-NLP', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'B-NLP', 'B-NLP', 'I-NLP', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'O', 'B-NLP', 'B-NLP', 'B-NLP', 'O', 'O', 'B-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'B-NLP', 'B-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'B-NLP', 'O', 'I-NLP', 'I-NLP', 'O', 'B-NLP', 'B-NLP', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'B-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'B-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'B-NLP', 'B-NLP', 'I-NLP', 'O', 'O', 'B-NLP', 'B-NLP', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'B-NLP', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'B-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'B-NLP', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'B-NLP', 'B-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'I-NLP', 'O', 'B-NLP', 'B-NLP', 'B-NLP', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'B-NLP', 'B-NLP', 'I-NLP', 'B-NLP', 'B-NLP', 'B-NLP', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'B-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'I-NLP', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'B-NLP', 'I-NLP', 'O', 'B-NLP', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'B-NLP', 'I-NLP', 'O', 'B-NLP', 'I-NLP', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-NLP', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'B-NLP', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'I-NLP', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'B-NLP', 'B-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'B-NLP', 'I-NLP', 'O', 'B-NLP', 'O', 'O', 'O', 'O', 'O', 'I-NLP', 'I-NLP', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'B-NLP', 'B-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'B-NLP', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'B-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'B-NLP', 'B-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'B-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'B-NLP', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'B-NLP', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'B-NLP', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'B-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'B-NLP', 'O', 'B-NLP', 'O', 'O', 'B-NLP', 'B-NLP', 'B-NLP', 'B-NLP', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'O', 'B-NLP', 'B-NLP', 'I-NLP', 'B-NLP', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-NLP', 'I-NLP', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'B-NLP', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'B-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'B-NLP', 'B-NLP', 'O', 'I-NLP', 'O', 'O', 'B-NLP', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'B-NLP', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'I-NLP', 'O', 'B-NLP', 'B-NLP', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'B-NLP', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'B-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'B-NLP', 'O', 'O', 'O', 'B-NLP', 'O', 'O', 'B-NLP', 'B-NLP', 'B-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'O', 'B-NLP', 'B-NLP', 'I-NLP', 'O', 'B-NLP', 'B-NLP', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'B-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'B-NLP', 'B-NLP', 'O', 'I-NLP', 'I-NLP', 'O', 'B-NLP', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'B-NLP', 'I-NLP', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'B-NLP', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'I-NLP', 'O', 'B-NLP', 'B-NLP', 'I-NLP', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'B-NLP', 'B-NLP', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'B-NLP', 'B-NLP', 'O', 'I-NLP', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'B-NLP', 'I-NLP', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'B-NLP', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'B-NLP', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-NLP', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'B-NLP', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'B-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'B-NLP', 'B-NLP', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'B-NLP', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O']\n",
            "Validation Loss: 0.13811150629003094\n",
            "Validation Accuracy: 0.9027107259333044\n"
          ]
        }
      ],
      "source": [
        "labels, predictions = valid(model, testing_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bClQwC0e-3Zk"
      },
      "source": [
        "# Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ypy4Myi-4xW",
        "outputId": "7b55fa0a-7e6d-4c74-c557-854bbd68afb2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['O', 'B-NLP', 'B-NLP', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'B-NLP', 'B-NLP', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'B-NLP', 'B-NLP', 'B-NLP', 'B-NLP', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'B-NLP', 'O', 'B-NLP', 'B-NLP', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'B-NLP', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'B-NLP', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'B-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'B-NLP', 'B-NLP', 'B-NLP', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'O', 'B-NLP', 'B-NLP', 'O', 'O', 'B-NLP', 'B-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'B-NLP', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'B-NLP', 'B-NLP', 'B-NLP', 'B-NLP', 'B-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'B-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'B-NLP', 'B-NLP', 'I-NLP', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'O', 'B-NLP', 'B-NLP', 'B-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'B-NLP', 'B-NLP', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'B-NLP', 'O', 'O', 'O', 'O', 'B-NLP', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'B-NLP', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'B-NLP', 'B-NLP', 'I-NLP', 'O', 'O', 'B-NLP', 'B-NLP', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'O', 'B-NLP', 'B-NLP', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'B-NLP', 'B-NLP', 'B-NLP', 'B-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'B-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'B-NLP', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'B-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'B-NLP', 'B-NLP', 'O', 'B-NLP', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'B-NLP', 'B-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'B-NLP', 'O', 'B-NLP', 'B-NLP', 'B-NLP', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'B-NLP', 'B-NLP', 'B-NLP', 'B-NLP', 'B-NLP', 'B-NLP', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'B-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'B-NLP', 'I-NLP', 'O', 'B-NLP', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'B-NLP', 'O', 'O', 'O', 'O', 'B-NLP', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'O', 'B-NLP', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'B-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'B-NLP', 'B-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'B-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'B-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'B-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'B-NLP', 'B-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'B-NLP', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'B-NLP', 'B-NLP', 'I-NLP', 'O', 'B-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'B-NLP', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'B-NLP', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'B-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'O', 'B-NLP', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'B-NLP', 'B-NLP', 'B-NLP', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'B-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'B-NLP', 'B-NLP', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'B-NLP', 'O', 'B-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'B-NLP', 'B-NLP', 'I-NLP', 'I-NLP', 'B-NLP', 'B-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'B-NLP', 'O', 'I-NLP', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'B-NLP', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'B-NLP', 'O', 'O', 'B-NLP', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'B-NLP', 'O', 'B-NLP', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'B-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'B-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'I-NLP', 'O', 'O', 'B-NLP', 'B-NLP', 'B-NLP', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'B-NLP', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'B-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'B-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'B-NLP', 'O', 'B-NLP', 'B-NLP', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'B-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'B-NLP', 'B-NLP', 'O', 'B-NLP', 'B-NLP', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'B-NLP', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'B-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'I-NLP', 'O', 'B-NLP', 'B-NLP', 'I-NLP', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'B-NLP', 'B-NLP', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'B-NLP', 'B-NLP', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'B-NLP', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'B-NLP', 'B-NLP', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'B-NLP', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'B-NLP', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'B-NLP', 'B-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'B-NLP', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'B-NLP', 'B-NLP', 'B-NLP', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'B-NLP', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'B-NLP', 'B-NLP', 'B-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O']\n",
            "['O', 'B-NLP', 'B-NLP', 'B-NLP', 'O', 'O', 'B-NLP', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'B-NLP', 'B-NLP', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'B-NLP', 'B-NLP', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'B-NLP', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'B-NLP', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'B-NLP', 'O', 'B-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'B-NLP', 'B-NLP', 'B-NLP', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'O', 'B-NLP', 'O', 'O', 'B-NLP', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'B-NLP', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'B-NLP', 'B-NLP', 'I-NLP', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'O', 'B-NLP', 'B-NLP', 'B-NLP', 'O', 'O', 'B-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'B-NLP', 'B-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'B-NLP', 'O', 'I-NLP', 'I-NLP', 'O', 'B-NLP', 'B-NLP', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'B-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'B-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'B-NLP', 'B-NLP', 'I-NLP', 'O', 'O', 'B-NLP', 'B-NLP', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'B-NLP', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'B-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'B-NLP', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'B-NLP', 'B-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'I-NLP', 'O', 'B-NLP', 'B-NLP', 'B-NLP', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'B-NLP', 'B-NLP', 'I-NLP', 'B-NLP', 'B-NLP', 'B-NLP', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'B-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'I-NLP', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'B-NLP', 'I-NLP', 'O', 'B-NLP', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'B-NLP', 'I-NLP', 'O', 'B-NLP', 'I-NLP', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-NLP', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'B-NLP', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'I-NLP', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'B-NLP', 'B-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'B-NLP', 'I-NLP', 'O', 'B-NLP', 'O', 'O', 'O', 'O', 'O', 'I-NLP', 'I-NLP', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'B-NLP', 'B-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'B-NLP', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'B-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'B-NLP', 'B-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'B-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'B-NLP', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'B-NLP', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'B-NLP', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'B-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'B-NLP', 'O', 'B-NLP', 'O', 'O', 'B-NLP', 'B-NLP', 'B-NLP', 'B-NLP', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'O', 'B-NLP', 'B-NLP', 'I-NLP', 'B-NLP', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-NLP', 'I-NLP', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'B-NLP', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'B-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'B-NLP', 'B-NLP', 'O', 'I-NLP', 'O', 'O', 'B-NLP', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'B-NLP', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'I-NLP', 'O', 'B-NLP', 'B-NLP', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'B-NLP', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'B-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'B-NLP', 'O', 'O', 'O', 'B-NLP', 'O', 'O', 'B-NLP', 'B-NLP', 'B-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'O', 'B-NLP', 'B-NLP', 'I-NLP', 'O', 'B-NLP', 'B-NLP', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'B-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'B-NLP', 'B-NLP', 'O', 'I-NLP', 'I-NLP', 'O', 'B-NLP', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'B-NLP', 'I-NLP', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'B-NLP', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'I-NLP', 'O', 'B-NLP', 'B-NLP', 'I-NLP', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'B-NLP', 'B-NLP', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'B-NLP', 'B-NLP', 'O', 'I-NLP', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'B-NLP', 'I-NLP', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'B-NLP', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'B-NLP', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-NLP', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'B-NLP', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'B-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'B-NLP', 'B-NLP', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'B-NLP', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O']\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         NLP       0.61      0.64      0.62       496\n",
            "\n",
            "   micro avg       0.61      0.64      0.62       496\n",
            "   macro avg       0.61      0.64      0.62       496\n",
            "weighted avg       0.61      0.64      0.62       496\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from seqeval.metrics import classification_report\n",
        "\n",
        "print(labels)\n",
        "print(predictions)\n",
        "\n",
        "print(classification_report([labels], [predictions]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-SCvV8MDafGQ",
        "outputId": "d5bd1d06-d1fd-4e59-d0f6-9c8e74511ae4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SENT Recent work in natural language generation has begun to take linguistic variation into account , developing algorithms that are capable of modifying the system 's linguistic style based either on the user 's linguistic style or other factors , such as personality or politeness .\n",
            "SENT While stylistic control has traditionally relied on handcrafted rules , statistical methods are likely to be needed for generation systems to scale to the production of the large range of variation observed in human dialogues .\n",
            "SENT Previous work on statistical natural language generation ( SNLG ) has shown that the grammaticality and naturalness of generated utterances can be optimized from data ; however these data - driven methods have not been shown to produce stylistic variation that is perceived by humans in the way that the system intended .\n",
            "SENT This paper describes Personage , a highly parameterizable language generator whose parameters are based on psychological findings about the linguistic reflexes of personality .\n",
            "SENT We present a novel SNLG method which uses parameter estimation models trained on personality - annotated data to predict the generation decisions required to convey any combination of scalar values along the five main dimensions of personality .\n",
            "SENT A human evaluation shows that parameter estimation models produce recognizable stylistic variation along multiple dimensions , on a continuous scale , and without the computational cost incurred by overgeneration techniques .\n",
            "SENT We present a statistical parsing framework for sentence - level sentiment classification in this article .\n",
            "SENT Unlike previous works that use syntactic parsing results for sentiment analysis , we develop a statistical parser to directly analyze the sentiment structure of a sentence .\n",
            "SENT We show that complicated phenomena in sentiment analysis ( e.g. , negation , intensification , and contrast ) can be handled the same way as simple and straightforward sentiment expressions in a unified and probabilistic way .\n",
            "SENT We formulate the sentiment grammar upon Context - Free Grammars ( CFGs ) , and provide a formal description of the sentiment parsing framework .\n",
            "SENT We develop the parsing model to obtain possible sentiment parse trees for a sentence , from which the polarity model is proposed to derive the sentiment strength and polarity , and the ranking model is dedicated to selecting the best sentiment tree .\n",
            "SENT We train the parser directly from examples of sentences annotated only with sentiment polarity labels but without any syntactic annotations or polarity annotations of constituents within sentences .\n",
            "SENT Therefore we can obtain training data easily .\n",
            "SENT In particular , we train a sentiment parser , s.parser , from a large amount of review sentences with users ' ratings as rough sentiment polarity labels .\n",
            "SENT Extensive experiments on existing benchmark data sets show significant improvements over baseline sentiment classification approaches .\n",
            "SENT The most common approach in text mining classification tasks is to rely on features like words , part - of - speech tags , stems , or some other high - level linguistic features .\n",
            "SENT Recently , an approach that uses only character p - grams as features has been proposed for the task of native language identification ( NLI ) .\n",
            "SENT The approach obtained state - of - the - art results by combining several string kernels using multiple kernel learning .\n",
            "SENT Despite the fact that the approach based on string kernels performs so well , several questions about this method remain unanswered .\n",
            "SENT First , it is not clear why such a simple approach can compete with far more complex approaches that take words , lemmas , syntactic information , or even semantics into account .\n",
            "SENT Second , although the approach is designed to be language independent , all experiments to date have been on English .\n",
            "SENT This work is an extensive study that aims to systematically present the string kernel approach and to clarify the open questions mentioned above .\n",
            "SENT A broad set of native language identification experiments were conducted to compare the string kernels approach with other state - of - the - art methods .\n",
            "SENT The empirical results obtained in all of the experiments conducted in this work indicate that the proposed approach achieves state - of - the - art performance in NLI , reaching an accuracy that is 1.7 % above the top scoring system of the 2013 NLI Shared Task .\n",
            "SENT Furthermore , the results obtained on both the Arabic and the Norwegian corpora demonstrate that the proposed approach is language independent .\n",
            "SENT In the Arabic native language identification task , string kernels show an increase of more than 17 % over the best accuracy reported so far .\n",
            "SENT The results of string kernels on Norwegian native language identification are also significantly better than the state - of - the - art approach .\n",
            "SENT In addition , in a cross - corpus experiment , the proposed approach shows that it can also be topic independent , improving the state - of - the - art system by 32.3 % .\n",
            "SENT To gain additional insights about the string kernels approach , the features selected by the classifier as being more discriminating are analyzed in this work .\n",
            "SENT The analysis also offers information about localized language transfer effects , since the features used by the proposed model are p - grams of various lengths .\n",
            "SENT The features captured by the model typically include stems , function words , and word prefixes and suffixes , which have the potential to generalize over purely word - based features .\n",
            "SENT By analyzing the discriminating features , this article offers insights into two kinds of language transfer effects , namely , word choice ( lexical transfer ) and morphological differences .\n",
            "SENT The goal of the current study is to give a full view of the string kernels approach and shed some light on why this approach works so well .\n",
            "SENT Many NLP applications entail that texts are classified based on their semantic distance ( how similar or different the texts are ) .\n",
            "SENT For example , comparing the text of a new document to that of documents of known topics can help identify the topic of the new text .\n",
            "SENT Typically , a distributional distance is used to capture the implicit semantic distance between two pieces of text .\n",
            "SENT However , such approaches do not take into account the semantic relations between words .\n",
            "SENT In this article , we introduce an alternative method of measuring the semantic distance between texts that integrates distributional information and ontological knowledge within a network flow formalism .\n",
            "SENT We first represent each text as a collection of frequency - weighted concepts within an ontology .\n",
            "SENT We then make use of a network flow method which provides an efficient way of explicitly measuring the frequency - weighted ontological distance between the concepts across two texts .\n",
            "SENT We evaluate our method in a variety of NLP tasks , and find that it performs well on two of three tasks .\n",
            "SENT We develop a new measure of semantic coherence that enables us to account for the performance difference across the three data sets , shedding light on the properties of a data set that lends itself well to our method .\n",
            "SENT This article deals with deverbal nominalizations in Spanish ; concretely , we focus on the denotative distinction between event and result nominalizations .\n",
            "SENT The goals of this work is twofold : ﬁrst , to detect the most relevant features for this denotative distinction ; and , second , to build an automatic classiﬁcation system of deverbal nominalizations according to their denotation .\n",
            "SENT We have based our study on theoretical hypotheses dealing with this semantic distinction and we have analyzed them empirically by means of Machine Learning techniques which are the basis of the ADN - Classiﬁer .\n",
            "SENT This is the ﬁrst tool that aims to automatically classify deverbal nominalizations in event , result , or underspeciﬁed denotation types in Spanish .\n",
            "SENT The ADN - Classiﬁer has helped us to quantitatively evaluate the validity of our claims regarding deverbal nominalizations .\n",
            "SENT We set up a series of experiments in order to test the ADN - Classiﬁer with different models and in different realistic scenarios depending on the knowledge resources and natural language processors available .\n",
            "SENT The ADN - Classiﬁer achieved good results ( 87.20 % accuracy ) .\n",
            "SENT Systems based on synchronous grammars and tree transducers promise to improve the quality of statistical machine translation output , but are often very computationally intensive .\n",
            "SENT The complexity is exponential in the size of individual grammar rules due to arbitrary re orderings between the two languages .\n",
            "SENT We develop a theory of binarization for synchronous context free grammars and present a linear time algorithm for binarizing synchronous rules when possible .\n",
            "SENT In our large scale experiments , we found that almost all rules are binarizable and the resulting binarized rule set significantly improves the speed and accuracy of a state of the art syntax based machine translation system .\n",
            "SENT We also discuss the more general , and computationally more difficult , problem of finding good parsing strategies for non binarizable rules , and present an approximate polynomial time algorithm for this problem .\n",
            "SENT We present algorithms for extracting Hyperedge Replacement Grammar ( HRG ) rules from a graph along with a vertex order .\n",
            "SENT Our algorithms are based on finding a tree decomposition of smallest width , relative to the vertex order , and then extracting one rule for each node in this structure .\n",
            "SENT The assumption of a fixed order for the vertices of the input graph makes it possible to solve the problem in polynomial time , in contrast to the fact that the problem of finding optimal tree decompositions for a graph is NP - hard .\n",
            "SENT We also present polynomial - time algorithms for parsing based on our HRGs , where the input is a vertex sequence and the output is a graph structure .\n",
            "SENT The intended application of our algorithms is grammar extraction and parsing for semantic representation of natural language .\n",
            "SENT We apply our algorithms to data annotated with Abstract Meaning Representations and report on the characteristics of the resulting grammars .\n",
            "SENT Weighted deduction systems provide a framework for describing parsing algorithms that can be used with a variety of operations for combining the values of partial derivations .\n",
            "SENT For some operations , inside values can be computed efficiently , but outside values can not .\n",
            "SENT We view out - side values as functions from inside values to the total value of all derivations , and we analyze outside computation in terms of function composition .\n",
            "SENT This viewpoint helps explain why efficient outside computation is possible in many settings , despite the lack of a general outside algorithm for semiring operations .\n",
            "SENT As more and more Arabic textual information becomes available through the Web in homes and businesses , via Internet and Intranet services , there is an urgent need for technologies and tools to process the relevant information .\n",
            "SENT Named Entity Recognition ( NER ) is an Information Extraction task that has become an integral part of many other Natural Language Processing ( NLP ) tasks , such as Machine Translation and Information Retrieval .\n",
            "SENT Arabic NER has begun to receive attention in recent years .\n",
            "SENT The characteristics and peculiarities of Arabic , a member of the Semitic languages family , make dealing with NER a challenge .\n",
            "SENT The performance of an Arabic NER component affects the overall performance of the NLP system in a positive manner .\n",
            "SENT This article attempts to describe and detail the recent increase in interest and progress made in Arabic NER research .\n",
            "SENT The importance of the NER task is demonstrated , the main characteristics of the Arabic language are highlighted , and the aspects of standardization in annotating named entities are illustrated .\n",
            "SENT Moreover , the different Arabic linguistic resources are presented and the approaches used in Arabic NER field are explained .\n",
            "SENT The features of common tools used in Arabic NER are described , and standard evaluation metrics are illustrated .\n",
            "SENT In addition , a review of the state of the art of Arabic NER research is discussed .\n",
            "SENT Finally , we present our conclusions .\n",
            "SENT Throughout the presentation , illustrative examples are used for clarification .\n",
            "SENT Translation models used for statistical machine translation are compiled from parallel corpora that are manually translated .\n",
            "SENT The common assumption is that parallel texts are symmetrical : The direction of translation is deemed irrelevant and is consequently ignored .\n",
            "SENT Much research in Translation Studies indicates that the direction of translation matters , however , as translated language ( translationese ) has many unique properties .\n",
            "SENT It has already been shown that phrase tables constructed from parallel corpora translated in the same direction as the translation task outperform those constructed from corpora translated in the opposite direction .\n",
            "SENT We reconfirm that this is indeed the case , but emphasize the importance of also using texts translated in the “ wrong ” direction .\n",
            "SENT We take advantage of information pertaining to the direction of translation in constructing phrase tables by adapting the translation model to the special properties of translationese .\n",
            "SENT We explore two adaptation techniques : First , we create a mixture model by interpolating phrase tables trained on texts translated in the “ right ” and the “ wrong ” directions .\n",
            "SENT The weights for the interpolation are determined by minimizing perplexity .\n",
            "SENT Second , we define entropy - based measures that estimate the correspondence of target - language phrases to translationese , thereby eliminating the need to annotate the parallel corpus with information pertaining to the direction of translation .\n",
            "SENT We show that incorporating these measures as features in the phrase tables of statistical machine translation systems results in consistent , statistically significant improvement in the quality of the translation .\n",
            "SENT Orthographic similarities across languages provide a strong signal for unsupervised probabilistic transduction ( decipherment ) for closely related language pairs .\n",
            "SENT The existing decipherment models , however , are not well suited for exploiting these orthographic similarities .\n",
            "SENT We propose a log - linear model with latent variables that incorporates orthographic similarity features .\n",
            "SENT Maximum likelihood training is computationally expensive for the proposed log - linear model .\n",
            "SENT To address this challenge , we perform approximate inference via Markov chain Monte Carlo sampling and contrastive divergence .\n",
            "SENT Our results show that the proposed log - linear model with contrastive divergence outperforms the existing generative decipherment models by exploiting the orthographic features .\n",
            "SENT The model both scales to large vocabularies and preserves accuracy in low- and no - resource contexts .\n",
            "SENT Although there has been much work in recent years on data - driven natural language generation , little attention has been paid to the fine - grained interactions that arise during microplanning between aggregation , surface realization , and sentence segmentation .\n",
            "SENT In this article , we propose a hybrid symbolic / statistical approach to jointly model the constraints regulating these interactions .\n",
            "SENT Our approach integrates a small handwritten grammar , a statistical hypertagger , and a surface realization algorithm .\n",
            "SENT It is applied to the verbalization of knowledge base queries and tested on 13 knowledge bases to demonstrate domain independence .\n",
            "SENT We evaluate our approach in several ways .\n",
            "SENT A quantitative analysis shows that the hybrid approach outperforms a purely symbolic approach in terms of both speed and coverage .\n",
            "SENT Results from a human study indicate that users find the output of this hybrid statistic / symbolic system more fluent than both a template - based and a purely symbolic grammar based approach .\n",
            "SENT Finally , we illustrate by means of examples that our approach can account for various factors impacting aggregation , sentence segmentation , and surface realization .\n",
            "SENT In this work , we present a phenomenon - oriented comparative analysis of the two dominant approaches in English Resource Semantic ( ERS ) parsing : classic , knowledge - intensive and neural , data - intensive models .\n",
            "SENT To reflect state - of - the - art neural NLP technologies , a factorization - based parser is introduced that can produce Elementary Dependency Structures much more accurately than previous data - driven parsers .\n",
            "SENT We conduct a suite of tests for different linguistic phenomena to analyze the grammatical competence of different parsers , where we show that , despite comparable performance overall , knowledge- and data - intensive models produce different types of errors , in a way that can be explained by their theoretical properties .\n",
            "SENT This analysis is beneficial to in - depth evaluation of several representative parsing techniques and leads to new directions for parser development .\n",
            "[[{'entity': 'B-NLP', 'score': 0.86076045, 'index': 4, 'word': 'natural', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.57448584, 'index': 5, 'word': 'language', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.8583303, 'index': 6, 'word': 'generation', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.9007368, 'index': 11, 'word': 'linguistic', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.5683373, 'index': 12, 'word': 'variation', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.8078602, 'index': 27, 'word': 'linguistic', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.6906469, 'index': 28, 'word': 'style', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.753112, 'index': 36, 'word': 'linguistic', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.6778252, 'index': 37, 'word': 'style', 'start': None, 'end': None}], [{'entity': 'B-NLP', 'score': 0.76794773, 'index': 2, 'word': 'stylistic', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.7258033, 'index': 3, 'word': 'control', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.70402926, 'index': 8, 'word': 'hand', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.6925627, 'index': 9, 'word': '##craft', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.55730754, 'index': 10, 'word': '##ed', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.6716081, 'index': 11, 'word': 'rules', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.6507656, 'index': 13, 'word': 'statistical', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.5450701, 'index': 14, 'word': 'methods', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.7184852, 'index': 21, 'word': 'generation', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.81619704, 'index': 22, 'word': 'systems', 'start': None, 'end': None}], [{'entity': 'B-NLP', 'score': 0.82428026, 'index': 4, 'word': 'statistical', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.7175466, 'index': 5, 'word': 'natural', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.84466475, 'index': 6, 'word': 'language', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.9106721, 'index': 7, 'word': 'generation', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.6298896, 'index': 9, 'word': 's', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.63133645, 'index': 10, 'word': '##nl', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.46905547, 'index': 11, 'word': '##g', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.85418785, 'index': 17, 'word': 'grammatical', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.42005596, 'index': 18, 'word': '##ity', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.6236536, 'index': 20, 'word': 'natural', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.47897205, 'index': 23, 'word': 'generated', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.64205575, 'index': 24, 'word': 'utter', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.5731663, 'index': 25, 'word': '##ances', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.78348935, 'index': 36, 'word': 'data', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.84794295, 'index': 37, 'word': '-', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.8016755, 'index': 38, 'word': 'driven', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.5497526, 'index': 39, 'word': 'methods', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.79693806, 'index': 46, 'word': 'stylistic', 'start': None, 'end': None}], [{'entity': 'B-NLP', 'score': 0.7895614, 'index': 4, 'word': 'persona', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.5453682, 'index': 5, 'word': '##ge', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.82672036, 'index': 9, 'word': 'parameter', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.5690102, 'index': 10, 'word': '##iza', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.7082442, 'index': 11, 'word': '##ble', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.54491276, 'index': 12, 'word': 'language', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.8641035, 'index': 13, 'word': 'generator', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.8396964, 'index': 23, 'word': 'linguistic', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.830565, 'index': 24, 'word': 'reflex', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.8212647, 'index': 25, 'word': '##es', 'start': None, 'end': None}], [{'entity': 'B-NLP', 'score': 0.8396647, 'index': 5, 'word': 's', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.7609621, 'index': 6, 'word': '##nl', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.5291776, 'index': 7, 'word': '##g', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.6369818, 'index': 8, 'word': 'method', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.8917184, 'index': 11, 'word': 'parameter', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.827167, 'index': 12, 'word': 'estimation', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.8041002, 'index': 13, 'word': 'models', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.8565956, 'index': 16, 'word': 'personality', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.7734284, 'index': 17, 'word': '-', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.7257711, 'index': 18, 'word': 'ann', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.689301, 'index': 19, 'word': '##ota', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.807822, 'index': 20, 'word': '##ted', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.8105793, 'index': 21, 'word': 'data', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.61667037, 'index': 25, 'word': 'generation', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.7260043, 'index': 33, 'word': 'scala', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.7954471, 'index': 34, 'word': '##r', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.691315, 'index': 35, 'word': 'values', 'start': None, 'end': None}], [{'entity': 'B-NLP', 'score': 0.6962883, 'index': 2, 'word': 'human', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.557638, 'index': 3, 'word': 'evaluation', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.87463415, 'index': 6, 'word': 'parameter', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.62857676, 'index': 7, 'word': 'estimation', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.76069385, 'index': 8, 'word': 'models', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.8344452, 'index': 11, 'word': 'stylistic', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.53754973, 'index': 12, 'word': 'variation', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.394615, 'index': 15, 'word': 'dimensions', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.56719065, 'index': 25, 'word': 'computational', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.62247247, 'index': 29, 'word': 'over', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.72894895, 'index': 30, 'word': '##gen', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.820059, 'index': 31, 'word': '##eration', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.6512021, 'index': 32, 'word': 'techniques', 'start': None, 'end': None}], [{'entity': 'B-NLP', 'score': 0.7816834, 'index': 4, 'word': 'statistical', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.88652366, 'index': 5, 'word': 'par', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.81529266, 'index': 6, 'word': '##sing', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.7546924, 'index': 7, 'word': 'framework', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.86847943, 'index': 9, 'word': 'sentence', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.91819423, 'index': 10, 'word': '-', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.8587998, 'index': 11, 'word': 'level', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.88545054, 'index': 12, 'word': 'sentiment', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.9239104, 'index': 13, 'word': 'classification', 'start': None, 'end': None}], [{'entity': 'B-NLP', 'score': 0.9263935, 'index': 6, 'word': 'syn', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.73935467, 'index': 7, 'word': '##ta', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.9006083, 'index': 8, 'word': '##ctic', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.89348716, 'index': 9, 'word': 'par', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.8515663, 'index': 10, 'word': '##sing', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.7692471, 'index': 11, 'word': 'results', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.8843234, 'index': 13, 'word': 'sentiment', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.88159806, 'index': 14, 'word': 'analysis', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.8928447, 'index': 19, 'word': 'statistical', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.86107075, 'index': 20, 'word': 'par', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.8273411, 'index': 21, 'word': '##ser', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.84850466, 'index': 26, 'word': 'sentiment', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.88593084, 'index': 27, 'word': 'structure', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.4575355, 'index': 30, 'word': 'sentence', 'start': None, 'end': None}], [{'entity': 'B-NLP', 'score': 0.8711924, 'index': 7, 'word': 'sentiment', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.89119685, 'index': 8, 'word': 'analysis', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.85587174, 'index': 15, 'word': 'ne', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.77131104, 'index': 16, 'word': '##gation', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.8167773, 'index': 18, 'word': 'int', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.7811048, 'index': 19, 'word': '##ens', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.60359067, 'index': 20, 'word': '##ification', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.65181977, 'index': 23, 'word': 'contrast', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.77886283, 'index': 35, 'word': 'sentiment', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.8801261, 'index': 36, 'word': 'expressions', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.83594894, 'index': 41, 'word': 'pro', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.813583, 'index': 42, 'word': '##ba', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.81076056, 'index': 43, 'word': '##bilis', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.67896307, 'index': 44, 'word': '##tic', 'start': None, 'end': None}], [{'entity': 'B-NLP', 'score': 0.90626687, 'index': 5, 'word': 'sentiment', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.9203344, 'index': 6, 'word': 'grammar', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.90512013, 'index': 8, 'word': 'context', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.9115979, 'index': 9, 'word': '-', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.86498046, 'index': 10, 'word': 'free', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.92670226, 'index': 11, 'word': 'grammar', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.9052321, 'index': 12, 'word': '##s', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.81834024, 'index': 14, 'word': 'cf', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.58719987, 'index': 15, 'word': '##gs', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.88321346, 'index': 25, 'word': 'sentiment', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.8836395, 'index': 26, 'word': 'par', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.8049805, 'index': 27, 'word': '##sing', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.8751775, 'index': 28, 'word': 'framework', 'start': None, 'end': None}], [{'entity': 'B-NLP', 'score': 0.8631436, 'index': 4, 'word': 'par', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.8132339, 'index': 5, 'word': '##sing', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.8452523, 'index': 6, 'word': 'model', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.88409907, 'index': 10, 'word': 'sentiment', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.8125446, 'index': 11, 'word': 'par', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.66182476, 'index': 12, 'word': '##se', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.92065513, 'index': 13, 'word': 'trees', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.53952223, 'index': 16, 'word': 'sentence', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.85266083, 'index': 21, 'word': 'polar', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.76908827, 'index': 22, 'word': '##ity', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.80919874, 'index': 23, 'word': 'model', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.80253977, 'index': 29, 'word': 'sentiment', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.6934899, 'index': 30, 'word': 'strength', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.7393136, 'index': 32, 'word': 'polar', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.5183262, 'index': 33, 'word': '##ity', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.68683213, 'index': 37, 'word': 'ranking', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.7505766, 'index': 38, 'word': 'model', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.83066833, 'index': 45, 'word': 'sentiment', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.8772358, 'index': 46, 'word': 'tree', 'start': None, 'end': None}], [{'entity': 'B-NLP', 'score': 0.6318401, 'index': 4, 'word': 'par', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.55651087, 'index': 5, 'word': '##ser', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.7125098, 'index': 10, 'word': 'sentences', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.51523316, 'index': 11, 'word': 'ann', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.49368718, 'index': 12, 'word': '##ota', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.8418073, 'index': 16, 'word': 'sentiment', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.8436719, 'index': 17, 'word': 'polar', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.82239836, 'index': 18, 'word': '##ity', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.84866637, 'index': 19, 'word': 'labels', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.919786, 'index': 23, 'word': 'syn', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.82882196, 'index': 24, 'word': '##ta', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.8613225, 'index': 25, 'word': '##ctic', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.79894376, 'index': 26, 'word': 'ann', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.70976603, 'index': 27, 'word': '##ota', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.6729826, 'index': 28, 'word': '##tions', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.67085654, 'index': 30, 'word': 'polar', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.6524405, 'index': 31, 'word': '##ity', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.8420376, 'index': 32, 'word': 'ann', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.7542737, 'index': 33, 'word': '##ota', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.7701954, 'index': 34, 'word': '##tions', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.57068115, 'index': 36, 'word': 'constituents', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.5303439, 'index': 38, 'word': 'sentences', 'start': None, 'end': None}], [{'entity': 'B-NLP', 'score': 0.6331427, 'index': 5, 'word': 'training', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.5345521, 'index': 6, 'word': 'data', 'start': None, 'end': None}], [{'entity': 'B-NLP', 'score': 0.85049003, 'index': 7, 'word': 'sentiment', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.88821113, 'index': 8, 'word': 'par', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.84716266, 'index': 9, 'word': '##ser', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.56650287, 'index': 12, 'word': '.', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.8269755, 'index': 13, 'word': 'par', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.7495884, 'index': 14, 'word': '##ser', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.7345299, 'index': 22, 'word': 'sentences', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.7717952, 'index': 29, 'word': 'sentiment', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.8495122, 'index': 30, 'word': 'polar', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.6998671, 'index': 31, 'word': '##ity', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.8503397, 'index': 32, 'word': 'labels', 'start': None, 'end': None}], [{'entity': 'B-NLP', 'score': 0.830707, 'index': 5, 'word': 'bench', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.56759447, 'index': 6, 'word': '##mark', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.89159197, 'index': 7, 'word': 'data', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.83167744, 'index': 8, 'word': 'sets', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.7715779, 'index': 13, 'word': 'baseline', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.7105399, 'index': 14, 'word': 'sentiment', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.9028958, 'index': 15, 'word': 'classification', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.7349566, 'index': 16, 'word': 'approaches', 'start': None, 'end': None}], [{'entity': 'B-NLP', 'score': 0.89036286, 'index': 6, 'word': 'text', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.7880021, 'index': 7, 'word': 'mining', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.7955422, 'index': 8, 'word': 'classification', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.8159653, 'index': 9, 'word': 'tasks', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.63842493, 'index': 16, 'word': 'words', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.66577923, 'index': 18, 'word': 'part', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.6692127, 'index': 19, 'word': '-', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.52940035, 'index': 20, 'word': 'of', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.67933106, 'index': 21, 'word': '-', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.73331213, 'index': 22, 'word': 'speech', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.7875709, 'index': 23, 'word': 'tags', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.75468206, 'index': 25, 'word': 'stems', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.50994337, 'index': 31, 'word': '-', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.444505, 'index': 32, 'word': 'level', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.6389877, 'index': 33, 'word': 'linguistic', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.52936465, 'index': 34, 'word': 'features', 'start': None, 'end': None}], [{'entity': 'B-NLP', 'score': 0.7744378, 'index': 8, 'word': 'character', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.51197404, 'index': 9, 'word': 'p', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.79186404, 'index': 10, 'word': '-', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.8142106, 'index': 11, 'word': 'grams', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.4474306, 'index': 13, 'word': 'features', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.8175958, 'index': 21, 'word': 'native', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.736944, 'index': 22, 'word': 'language', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.8255493, 'index': 23, 'word': 'identification', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.80980563, 'index': 25, 'word': 'nl', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.59898144, 'index': 26, 'word': '##i', 'start': None, 'end': None}], [{'entity': 'B-NLP', 'score': 0.67175937, 'index': 4, 'word': 'state', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.77028143, 'index': 5, 'word': '-', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.72177374, 'index': 6, 'word': 'of', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.7586791, 'index': 7, 'word': '-', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.65668875, 'index': 8, 'word': 'the', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.8071173, 'index': 9, 'word': '-', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.36038655, 'index': 10, 'word': 'art', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.86998755, 'index': 15, 'word': 'string', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.8979978, 'index': 16, 'word': 'kernel', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.8268194, 'index': 17, 'word': '##s', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.47640285, 'index': 20, 'word': 'kernel', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.89566344, 'index': 21, 'word': 'learning', 'start': None, 'end': None}], [{'entity': 'B-NLP', 'score': 0.8643072, 'index': 9, 'word': 'string', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.8785612, 'index': 10, 'word': 'kernel', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.8231734, 'index': 11, 'word': '##s', 'start': None, 'end': None}], [{'entity': 'B-NLP', 'score': 0.7027014, 'index': 21, 'word': 'words', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.83344144, 'index': 23, 'word': 'le', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.7193723, 'index': 24, 'word': '##mma', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.5427156, 'index': 25, 'word': '##s', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.89366984, 'index': 27, 'word': 'syn', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.80126566, 'index': 28, 'word': '##ta', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.8617805, 'index': 29, 'word': '##ctic', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.75059676, 'index': 30, 'word': 'information', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.62312716, 'index': 34, 'word': 'semantics', 'start': None, 'end': None}], [{'entity': 'B-NLP', 'score': 0.80300653, 'index': 10, 'word': 'language', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.5434772, 'index': 11, 'word': 'independent', 'start': None, 'end': None}], [{'entity': 'B-NLP', 'score': 0.87566507, 'index': 13, 'word': 'string', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.843737, 'index': 14, 'word': 'kernel', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.73464996, 'index': 15, 'word': 'approach', 'start': None, 'end': None}], [{'entity': 'B-NLP', 'score': 0.8406934, 'index': 5, 'word': 'native', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.7123918, 'index': 6, 'word': 'language', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.8910228, 'index': 7, 'word': 'identification', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.8820541, 'index': 14, 'word': 'string', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.86665434, 'index': 15, 'word': 'kernel', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.8744655, 'index': 16, 'word': '##s', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.5656988, 'index': 17, 'word': 'approach', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.58797824, 'index': 20, 'word': 'state', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.7953237, 'index': 21, 'word': '-', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.71605355, 'index': 22, 'word': 'of', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.7754258, 'index': 23, 'word': '-', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.66414267, 'index': 24, 'word': 'the', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.82388437, 'index': 25, 'word': '-', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.37479118, 'index': 26, 'word': 'art', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.5746665, 'index': 27, 'word': 'methods', 'start': None, 'end': None}], [{'entity': 'I-NLP', 'score': 0.7183782, 'index': 22, 'word': '-', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.55807716, 'index': 23, 'word': 'of', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.7211406, 'index': 24, 'word': '-', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.60909426, 'index': 25, 'word': 'the', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.73517245, 'index': 26, 'word': '-', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.80429, 'index': 30, 'word': 'nl', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.58679587, 'index': 31, 'word': '##i', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.44337612, 'index': 46, 'word': 'system', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.850871, 'index': 50, 'word': 'nl', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.6653648, 'index': 51, 'word': '##i', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.4049253, 'index': 52, 'word': 'shared', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.5493645, 'index': 53, 'word': 'task', 'start': None, 'end': None}], [{'entity': 'I-NLP', 'score': 0.5449632, 'index': 13, 'word': 'corp', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.6968191, 'index': 14, 'word': '##ora', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.80742055, 'index': 21, 'word': 'language', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.6450988, 'index': 22, 'word': 'independent', 'start': None, 'end': None}], [{'entity': 'B-NLP', 'score': 0.58070207, 'index': 3, 'word': 'arabic', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.5071495, 'index': 4, 'word': 'native', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.6591353, 'index': 5, 'word': 'language', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.876184, 'index': 6, 'word': 'identification', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.67566216, 'index': 7, 'word': 'task', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.82987636, 'index': 9, 'word': 'string', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.81394225, 'index': 10, 'word': 'kernel', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.56824654, 'index': 11, 'word': '##s', 'start': None, 'end': None}], [{'entity': 'B-NLP', 'score': 0.8736245, 'index': 4, 'word': 'string', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.863154, 'index': 5, 'word': 'kernel', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.8139217, 'index': 6, 'word': '##s', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.5400774, 'index': 9, 'word': 'native', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.6424691, 'index': 10, 'word': 'language', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.9068527, 'index': 11, 'word': 'identification', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.5466177, 'index': 18, 'word': 'state', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.7642925, 'index': 19, 'word': '-', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.6902911, 'index': 20, 'word': 'of', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.755177, 'index': 21, 'word': '-', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.619682, 'index': 22, 'word': 'the', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.7921597, 'index': 23, 'word': '-', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.40811393, 'index': 24, 'word': 'art', 'start': None, 'end': None}], [{'entity': 'B-NLP', 'score': 0.822376, 'index': 6, 'word': 'cross', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.83728075, 'index': 7, 'word': '-', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.4854531, 'index': 8, 'word': 'corpus', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.6580318, 'index': 9, 'word': 'experiment', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.83245903, 'index': 20, 'word': 'topic', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.571985, 'index': 21, 'word': 'independent', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.5416768, 'index': 25, 'word': 'state', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.7645685, 'index': 26, 'word': '-', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.669196, 'index': 27, 'word': 'of', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.7540052, 'index': 28, 'word': '-', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.5761585, 'index': 29, 'word': 'the', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.7798526, 'index': 30, 'word': '-', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.43450052, 'index': 31, 'word': 'art', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.7112424, 'index': 32, 'word': 'system', 'start': None, 'end': None}], [{'entity': 'B-NLP', 'score': 0.8892631, 'index': 7, 'word': 'string', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.84043235, 'index': 8, 'word': 'kernel', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.82250607, 'index': 9, 'word': '##s', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.8453994, 'index': 17, 'word': 'class', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.46607572, 'index': 18, 'word': '##ifier', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.6104518, 'index': 22, 'word': 'disc', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.59874547, 'index': 23, 'word': '##rim', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.5031825, 'index': 24, 'word': '##inating', 'start': None, 'end': None}], [{'entity': 'B-NLP', 'score': 0.80311775, 'index': 7, 'word': 'localized', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.5869607, 'index': 8, 'word': 'language', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.9026404, 'index': 9, 'word': 'transfer', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.89042485, 'index': 10, 'word': 'effects', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.8424476, 'index': 21, 'word': 'p', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.758711, 'index': 22, 'word': '-', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.7091149, 'index': 23, 'word': 'grams', 'start': None, 'end': None}], [{'entity': 'B-NLP', 'score': 0.8080297, 'index': 9, 'word': 'stems', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.8564171, 'index': 11, 'word': 'function', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.8118363, 'index': 12, 'word': 'words', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.81115294, 'index': 15, 'word': 'word', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.79575974, 'index': 16, 'word': 'prefix', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.79273057, 'index': 17, 'word': '##es', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.43005377, 'index': 19, 'word': 'suffix', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.7495181, 'index': 20, 'word': '##es', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.84094733, 'index': 31, 'word': 'word', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.85610294, 'index': 32, 'word': '-', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.8047564, 'index': 33, 'word': 'based', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.5701225, 'index': 34, 'word': 'features', 'start': None, 'end': None}], [{'entity': 'B-NLP', 'score': 0.7830083, 'index': 4, 'word': 'disc', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.81185323, 'index': 5, 'word': '##rim', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.8714544, 'index': 6, 'word': '##inating', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.70317835, 'index': 7, 'word': 'features', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.852682, 'index': 17, 'word': 'language', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.87326497, 'index': 18, 'word': 'transfer', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.7579752, 'index': 19, 'word': 'effects', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.85584515, 'index': 23, 'word': 'word', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.71479833, 'index': 24, 'word': 'choice', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.87614596, 'index': 26, 'word': 'lexi', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.86581707, 'index': 27, 'word': '##cal', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.8939216, 'index': 28, 'word': 'transfer', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.8728808, 'index': 31, 'word': 'morphological', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.69164944, 'index': 32, 'word': 'differences', 'start': None, 'end': None}], [{'entity': 'B-NLP', 'score': 0.84828854, 'index': 15, 'word': 'string', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.85730606, 'index': 16, 'word': 'kernel', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.8545413, 'index': 17, 'word': '##s', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.50685817, 'index': 18, 'word': 'approach', 'start': None, 'end': None}], [{'entity': 'B-NLP', 'score': 0.88859504, 'index': 2, 'word': 'nl', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.8129144, 'index': 3, 'word': '##p', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.8640987, 'index': 14, 'word': 'semantic', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.69567454, 'index': 15, 'word': 'distance', 'start': None, 'end': None}], [], [{'entity': 'B-NLP', 'score': 0.89993304, 'index': 4, 'word': 'distribution', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.85484284, 'index': 5, 'word': '##al', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.863725, 'index': 6, 'word': 'distance', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.7415901, 'index': 12, 'word': 'implicit', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.5079981, 'index': 13, 'word': 'semantic', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.8485959, 'index': 14, 'word': 'distance', 'start': None, 'end': None}], [{'entity': 'B-NLP', 'score': 0.84130746, 'index': 11, 'word': 'semantic', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.82743347, 'index': 12, 'word': 'relations', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.49034607, 'index': 14, 'word': 'words', 'start': None, 'end': None}], [{'entity': 'B-NLP', 'score': 0.84110385, 'index': 13, 'word': 'semantic', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.7998286, 'index': 14, 'word': 'distance', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.5640104, 'index': 16, 'word': 'texts', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.8931485, 'index': 20, 'word': 'distribution', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.8230125, 'index': 21, 'word': '##al', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.8331131, 'index': 22, 'word': 'information', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.8587454, 'index': 24, 'word': 'onto', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.89223576, 'index': 25, 'word': '##logical', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.80359644, 'index': 26, 'word': 'knowledge', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.8596415, 'index': 29, 'word': 'network', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.8501192, 'index': 30, 'word': 'flow', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.8706672, 'index': 31, 'word': 'formal', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.8997179, 'index': 32, 'word': '##ism', 'start': None, 'end': None}], [{'entity': 'B-NLP', 'score': 0.9007054, 'index': 10, 'word': 'frequency', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.88988674, 'index': 11, 'word': '-', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.87142366, 'index': 12, 'word': 'weighted', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.5866169, 'index': 13, 'word': 'concepts', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.75182205, 'index': 16, 'word': 'onto', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.45316446, 'index': 17, 'word': '##logy', 'start': None, 'end': None}], [{'entity': 'B-NLP', 'score': 0.83367366, 'index': 7, 'word': 'network', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.9089239, 'index': 8, 'word': 'flow', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.7633981, 'index': 9, 'word': 'method', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.90399003, 'index': 19, 'word': 'frequency', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.8761175, 'index': 20, 'word': '-', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.85813606, 'index': 21, 'word': 'weighted', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.5560763, 'index': 22, 'word': 'onto', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.57480603, 'index': 23, 'word': '##logical', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.83318114, 'index': 24, 'word': 'distance', 'start': None, 'end': None}], [{'entity': 'B-NLP', 'score': 0.9016, 'index': 9, 'word': 'nl', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.71337, 'index': 10, 'word': '##p', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.6439855, 'index': 11, 'word': 'tasks', 'start': None, 'end': None}], [{'entity': 'B-NLP', 'score': 0.89801127, 'index': 7, 'word': 'semantic', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.8542216, 'index': 8, 'word': 'co', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.5098822, 'index': 9, 'word': '##her', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.84005266, 'index': 10, 'word': '##ence', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.6102382, 'index': 18, 'word': 'performance', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.6674231, 'index': 23, 'word': 'data', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.6471766, 'index': 24, 'word': 'sets', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.7353391, 'index': 34, 'word': 'data', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.6908537, 'index': 35, 'word': 'set', 'start': None, 'end': None}], [{'entity': 'B-NLP', 'score': 0.8069144, 'index': 5, 'word': 'dev', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.6940366, 'index': 6, 'word': '##er', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.8331776, 'index': 7, 'word': '##bal', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.8221073, 'index': 8, 'word': 'nominal', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.8885843, 'index': 9, 'word': '##izations', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.7314064, 'index': 20, 'word': 'den', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.8393372, 'index': 21, 'word': '##ota', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.7924263, 'index': 22, 'word': '##tive', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.6636578, 'index': 23, 'word': 'distinction', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.61415434, 'index': 25, 'word': 'event', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.66441554, 'index': 27, 'word': 'result', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.7846711, 'index': 28, 'word': 'nominal', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.861653, 'index': 29, 'word': '##izations', 'start': None, 'end': None}], [{'entity': 'B-NLP', 'score': 0.5864285, 'index': 21, 'word': 'den', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.78655475, 'index': 22, 'word': '##ota', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.66211766, 'index': 23, 'word': '##tive', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.88870114, 'index': 24, 'word': 'distinction', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.6529337, 'index': 33, 'word': 'automatic', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.68092155, 'index': 34, 'word': 'class', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.48057187, 'index': 35, 'word': '##i', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.48303732, 'index': 37, 'word': '##cation', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.83210117, 'index': 38, 'word': 'system', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.726318, 'index': 40, 'word': 'dev', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.54536, 'index': 41, 'word': '##er', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.7304004, 'index': 42, 'word': '##bal', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.78343743, 'index': 43, 'word': 'nominal', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.829345, 'index': 44, 'word': '##izations', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.5129858, 'index': 48, 'word': 'den', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.75365824, 'index': 49, 'word': '##ota', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.52436924, 'index': 50, 'word': '##tion', 'start': None, 'end': None}], [{'entity': 'B-NLP', 'score': 0.68237966, 'index': 7, 'word': 'theoretical', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.6676724, 'index': 8, 'word': 'h', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.67113143, 'index': 9, 'word': '##yp', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.721659, 'index': 10, 'word': '##oth', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.82294035, 'index': 15, 'word': 'semantic', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.77043945, 'index': 16, 'word': 'distinction', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.8633083, 'index': 27, 'word': 'machine', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.8401773, 'index': 28, 'word': 'learning', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.71759385, 'index': 29, 'word': 'techniques', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.7768197, 'index': 36, 'word': 'ad', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.8206528, 'index': 37, 'word': '##n', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.6735062, 'index': 38, 'word': '-', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.52948636, 'index': 39, 'word': 'class', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.54317576, 'index': 40, 'word': '##i', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.8233433, 'index': 42, 'word': '##er', 'start': None, 'end': None}], [{'entity': 'B-NLP', 'score': 0.8029066, 'index': 5, 'word': '##rst', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.7621897, 'index': 12, 'word': 'dev', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.59522855, 'index': 13, 'word': '##er', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.72948414, 'index': 14, 'word': '##bal', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.7939756, 'index': 15, 'word': 'nominal', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.85147876, 'index': 16, 'word': '##izations', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.66757834, 'index': 18, 'word': 'event', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.46185797, 'index': 20, 'word': 'result', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.591622, 'index': 23, 'word': 'under', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.47156003, 'index': 24, 'word': '##sp', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.52731735, 'index': 25, 'word': '##ec', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.61065143, 'index': 26, 'word': '##i', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.54092026, 'index': 28, 'word': '##ed', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.8291653, 'index': 29, 'word': 'den', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.613603, 'index': 30, 'word': '##ota', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.7773806, 'index': 31, 'word': '##tion', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.894116, 'index': 32, 'word': 'types', 'start': None, 'end': None}], [{'entity': 'B-NLP', 'score': 0.7616215, 'index': 2, 'word': 'ad', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.82386816, 'index': 3, 'word': '##n', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.6505324, 'index': 4, 'word': '-', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.5181301, 'index': 5, 'word': 'class', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.56411046, 'index': 6, 'word': '##i', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.7917695, 'index': 8, 'word': '##er', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.7254049, 'index': 22, 'word': 'dev', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.5258556, 'index': 23, 'word': '##er', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.761441, 'index': 24, 'word': '##bal', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.8488904, 'index': 25, 'word': 'nominal', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.9028136, 'index': 26, 'word': '##izations', 'start': None, 'end': None}], [{'entity': 'B-NLP', 'score': 0.79258174, 'index': 13, 'word': 'ad', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.75156295, 'index': 14, 'word': '##n', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.7348461, 'index': 15, 'word': '-', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.65348107, 'index': 16, 'word': 'class', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.55223596, 'index': 17, 'word': '##i', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.8432479, 'index': 19, 'word': '##er', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.7450746, 'index': 31, 'word': 'knowledge', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.6574594, 'index': 32, 'word': 'resources', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.81781936, 'index': 34, 'word': 'natural', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.5927236, 'index': 35, 'word': 'language', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.8220649, 'index': 36, 'word': 'processors', 'start': None, 'end': None}], [{'entity': 'B-NLP', 'score': 0.6989292, 'index': 2, 'word': 'ad', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.74365485, 'index': 3, 'word': '##n', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.7153332, 'index': 4, 'word': '-', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.58911484, 'index': 5, 'word': 'class', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.45086506, 'index': 6, 'word': '##i', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.6415553, 'index': 8, 'word': '##er', 'start': None, 'end': None}], [{'entity': 'B-NLP', 'score': 0.8782515, 'index': 4, 'word': 'sync', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.70182997, 'index': 5, 'word': '##hr', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.72687393, 'index': 6, 'word': '##ono', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.80911094, 'index': 7, 'word': '##us', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.91643983, 'index': 8, 'word': 'grammar', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.8587337, 'index': 9, 'word': '##s', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.8952527, 'index': 11, 'word': 'tree', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.8843721, 'index': 12, 'word': 'trans', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.7774422, 'index': 13, 'word': '##du', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.8823726, 'index': 14, 'word': '##cer', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.87556785, 'index': 15, 'word': '##s', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.8364193, 'index': 22, 'word': 'statistical', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.8197006, 'index': 23, 'word': 'machine', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.8554579, 'index': 24, 'word': 'translation', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.77905625, 'index': 25, 'word': 'output', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.6885411, 'index': 31, 'word': 'computational', 'start': None, 'end': None}], [{'entity': 'B-NLP', 'score': 0.824867, 'index': 10, 'word': 'grammar', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.8488261, 'index': 11, 'word': 'rules', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.7562812, 'index': 15, 'word': 're', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.7956414, 'index': 16, 'word': 'ordering', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.6672691, 'index': 17, 'word': '##s', 'start': None, 'end': None}], [{'entity': 'B-NLP', 'score': 0.90128565, 'index': 6, 'word': 'bin', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.5900631, 'index': 7, 'word': '##ari', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.6482871, 'index': 8, 'word': '##zation', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.8937038, 'index': 10, 'word': 'sync', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.7271977, 'index': 11, 'word': '##hr', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.6723385, 'index': 12, 'word': '##ono', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.78209233, 'index': 13, 'word': '##us', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.81168115, 'index': 14, 'word': 'context', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.90943426, 'index': 15, 'word': 'free', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.93608147, 'index': 16, 'word': 'grammar', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.9146572, 'index': 17, 'word': '##s', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.8436492, 'index': 21, 'word': 'linear', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.7522573, 'index': 22, 'word': 'time', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.84801257, 'index': 23, 'word': 'algorithm', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.9026676, 'index': 25, 'word': 'bin', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.6380399, 'index': 26, 'word': '##ari', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.37752646, 'index': 27, 'word': '##zing', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.8493815, 'index': 28, 'word': 'sync', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.6570106, 'index': 29, 'word': '##hr', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.5955506, 'index': 30, 'word': '##ono', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.5861196, 'index': 31, 'word': '##us', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.858812, 'index': 32, 'word': 'rules', 'start': None, 'end': None}], [{'entity': 'B-NLP', 'score': 0.6118696, 'index': 12, 'word': 'rules', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.8608722, 'index': 14, 'word': 'bin', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.6005259, 'index': 15, 'word': '##ari', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.45215425, 'index': 16, 'word': '##za', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.42401734, 'index': 17, 'word': '##ble', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.8915461, 'index': 21, 'word': 'bin', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.6616161, 'index': 22, 'word': '##ari', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.62427944, 'index': 23, 'word': '##zed', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.56788, 'index': 24, 'word': 'rule', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.72514766, 'index': 25, 'word': 'set', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.6240676, 'index': 35, 'word': 'of', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.5345856, 'index': 36, 'word': 'the', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.6461607, 'index': 38, 'word': 'syntax', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.84756505, 'index': 39, 'word': 'based', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.70091444, 'index': 40, 'word': 'machine', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.86833745, 'index': 41, 'word': 'translation', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.8514948, 'index': 42, 'word': 'system', 'start': None, 'end': None}], [{'entity': 'B-NLP', 'score': 0.53168064, 'index': 9, 'word': 'computational', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.78578186, 'index': 18, 'word': 'par', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.7544006, 'index': 19, 'word': '##sing', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.7912978, 'index': 20, 'word': 'strategies', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.6068954, 'index': 22, 'word': 'non', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.7503062, 'index': 23, 'word': 'bin', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.6524713, 'index': 24, 'word': '##ari', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.55032384, 'index': 25, 'word': '##za', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.49560744, 'index': 26, 'word': '##ble', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.79910785, 'index': 27, 'word': 'rules', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.70200956, 'index': 32, 'word': 'approximate', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.5046512, 'index': 33, 'word': 'polynomial', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.7426478, 'index': 34, 'word': 'time', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.76640123, 'index': 35, 'word': 'algorithm', 'start': None, 'end': None}], [{'entity': 'B-NLP', 'score': 0.89827186, 'index': 7, 'word': 'hyper', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.8495345, 'index': 8, 'word': '##edge', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.87281704, 'index': 9, 'word': 'replacement', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.9041601, 'index': 10, 'word': 'grammar', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.821885, 'index': 12, 'word': 'hr', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.6348729, 'index': 13, 'word': '##g', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.76382667, 'index': 15, 'word': 'rules', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.51376, 'index': 18, 'word': 'graph', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.7810479, 'index': 22, 'word': 'vertex', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.85991275, 'index': 23, 'word': 'order', 'start': None, 'end': None}], [{'entity': 'B-NLP', 'score': 0.8925373, 'index': 8, 'word': 'tree', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.8147412, 'index': 9, 'word': 'decomposition', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.37924665, 'index': 12, 'word': 'width', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.7960819, 'index': 17, 'word': 'vertex', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.7621363, 'index': 18, 'word': 'order', 'start': None, 'end': None}], [{'entity': 'B-NLP', 'score': 0.6118481, 'index': 5, 'word': 'fixed', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.597625, 'index': 6, 'word': 'order', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.6702547, 'index': 12, 'word': 'input', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.4943497, 'index': 13, 'word': 'graph', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.5399101, 'index': 22, 'word': 'polynomial', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.46750268, 'index': 23, 'word': 'time', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.8420598, 'index': 35, 'word': 'optimal', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.703288, 'index': 36, 'word': 'tree', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.88104546, 'index': 37, 'word': 'decomposition', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.8303329, 'index': 38, 'word': '##s', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.47913012, 'index': 41, 'word': 'graph', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.8248265, 'index': 43, 'word': 'np', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.68222094, 'index': 44, 'word': '-', 'start': None, 'end': None}], [{'entity': 'B-NLP', 'score': 0.85090363, 'index': 4, 'word': 'polynomial', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.7766934, 'index': 5, 'word': '-', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.6539376, 'index': 6, 'word': 'time', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.6709093, 'index': 7, 'word': 'algorithms', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.88371825, 'index': 9, 'word': 'par', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.7389231, 'index': 10, 'word': '##sing', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.83886665, 'index': 14, 'word': 'hr', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.4696604, 'index': 15, 'word': '##gs', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.50778186, 'index': 19, 'word': 'input', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.78394496, 'index': 22, 'word': 'vertex', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.7918598, 'index': 23, 'word': 'sequence', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.8112517, 'index': 29, 'word': 'graph', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.7942035, 'index': 30, 'word': 'structure', 'start': None, 'end': None}], [{'entity': 'B-NLP', 'score': 0.9148009, 'index': 8, 'word': 'grammar', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.8302849, 'index': 9, 'word': 'extraction', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.7336746, 'index': 11, 'word': 'par', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.56931984, 'index': 12, 'word': '##sing', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.8995151, 'index': 14, 'word': 'semantic', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.8816208, 'index': 15, 'word': 'representation', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.817579, 'index': 17, 'word': 'natural', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.59313154, 'index': 18, 'word': 'language', 'start': None, 'end': None}], [{'entity': 'B-NLP', 'score': 0.6574964, 'index': 6, 'word': 'data', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.5047909, 'index': 7, 'word': 'ann', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.46711782, 'index': 8, 'word': '##ota', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.7554207, 'index': 11, 'word': 'abstract', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.47027966, 'index': 12, 'word': 'meaning', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.9065401, 'index': 13, 'word': 'representations', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.64276356, 'index': 22, 'word': 'grammar', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.4681455, 'index': 23, 'word': '##s', 'start': None, 'end': None}], [{'entity': 'B-NLP', 'score': 0.79336923, 'index': 1, 'word': 'weighted', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.70287347, 'index': 2, 'word': 'de', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.58884615, 'index': 3, 'word': '##duction', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.8287979, 'index': 4, 'word': 'systems', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.8796426, 'index': 10, 'word': 'par', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.85702974, 'index': 11, 'word': '##sing', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.7564317, 'index': 12, 'word': 'algorithms', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.8100226, 'index': 27, 'word': 'partial', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.5986516, 'index': 28, 'word': 'derivation', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.68851537, 'index': 29, 'word': '##s', 'start': None, 'end': None}], [], [{'entity': 'B-NLP', 'score': 0.60188246, 'index': 3, 'word': 'out', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.3776745, 'index': 4, 'word': '-', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.5266179, 'index': 5, 'word': 'side', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.48255557, 'index': 6, 'word': 'values', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.67174774, 'index': 18, 'word': 'derivation', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.78496885, 'index': 29, 'word': 'function', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.6833035, 'index': 30, 'word': 'composition', 'start': None, 'end': None}], [{'entity': 'B-NLP', 'score': 0.6331085, 'index': 7, 'word': 'outside', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.6699675, 'index': 8, 'word': 'computation', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.48725805, 'index': 21, 'word': 'outside', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.78469944, 'index': 22, 'word': 'algorithm', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.84873813, 'index': 24, 'word': 'semi', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.8172854, 'index': 25, 'word': '##ring', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.82222867, 'index': 26, 'word': 'operations', 'start': None, 'end': None}], [{'entity': 'B-NLP', 'score': 0.57989407, 'index': 6, 'word': 'textual', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.4508484, 'index': 7, 'word': 'information', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.6387451, 'index': 21, 'word': 'intra', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.6479388, 'index': 22, 'word': '##net', 'start': None, 'end': None}], [{'entity': 'B-NLP', 'score': 0.6127465, 'index': 1, 'word': 'named', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.47330922, 'index': 2, 'word': 'entity', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.85454345, 'index': 3, 'word': 'recognition', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.74476933, 'index': 5, 'word': 'ne', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.63564634, 'index': 6, 'word': '##r', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.7575874, 'index': 10, 'word': 'information', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.75825137, 'index': 11, 'word': 'extraction', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.8203323, 'index': 12, 'word': 'task', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.7905489, 'index': 22, 'word': 'natural', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.6782995, 'index': 23, 'word': 'language', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.8819014, 'index': 24, 'word': 'processing', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.80193406, 'index': 26, 'word': 'nl', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.5592234, 'index': 27, 'word': '##p', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.72868925, 'index': 29, 'word': 'tasks', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.8299761, 'index': 33, 'word': 'machine', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.817313, 'index': 34, 'word': 'translation', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.72104704, 'index': 36, 'word': 'information', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.6904158, 'index': 37, 'word': 'retrieval', 'start': None, 'end': None}], [{'entity': 'B-NLP', 'score': 0.63176376, 'index': 2, 'word': 'ne', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.49270174, 'index': 3, 'word': '##r', 'start': None, 'end': None}], [{'entity': 'B-NLP', 'score': 0.5419601, 'index': 13, 'word': 'semitic', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.44660154, 'index': 14, 'word': 'languages', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.51906496, 'index': 15, 'word': 'family', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.5716343, 'index': 20, 'word': 'ne', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.61769706, 'index': 21, 'word': '##r', 'start': None, 'end': None}], [{'entity': 'B-NLP', 'score': 0.7242526, 'index': 5, 'word': 'arabic', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.62726545, 'index': 6, 'word': 'ne', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.49434882, 'index': 7, 'word': '##r', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.6777959, 'index': 8, 'word': 'component', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.88787997, 'index': 15, 'word': 'nl', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.61040014, 'index': 16, 'word': '##p', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.70676273, 'index': 17, 'word': 'system', 'start': None, 'end': None}], [{'entity': 'B-NLP', 'score': 0.6582527, 'index': 17, 'word': 'arabic', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.6451306, 'index': 18, 'word': 'ne', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.5137217, 'index': 19, 'word': '##r', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.480994, 'index': 20, 'word': 'research', 'start': None, 'end': None}], [{'entity': 'B-NLP', 'score': 0.8596594, 'index': 5, 'word': 'ne', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.82341516, 'index': 6, 'word': '##r', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.46656874, 'index': 7, 'word': 'task', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.3575367, 'index': 17, 'word': 'language', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.8066636, 'index': 27, 'word': 'ann', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.75775856, 'index': 28, 'word': '##ota', 'start': None, 'end': None}], [{'entity': 'B-NLP', 'score': 0.62622565, 'index': 5, 'word': 'arabic', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.49504098, 'index': 6, 'word': 'linguistic', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.7397572, 'index': 7, 'word': 'resources', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.5077802, 'index': 15, 'word': 'arabic', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.6208637, 'index': 16, 'word': 'ne', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.49439445, 'index': 17, 'word': '##r', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.62730646, 'index': 18, 'word': 'field', 'start': None, 'end': None}], [{'entity': 'B-NLP', 'score': 0.62705773, 'index': 8, 'word': 'arabic', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.73294026, 'index': 9, 'word': 'ne', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.47066247, 'index': 10, 'word': '##r', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.6776984, 'index': 15, 'word': 'standard', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.53511786, 'index': 16, 'word': 'evaluation', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.88119036, 'index': 17, 'word': 'metric', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.8783338, 'index': 18, 'word': '##s', 'start': None, 'end': None}], [{'entity': 'B-NLP', 'score': 0.66895306, 'index': 13, 'word': 'arabic', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.6046114, 'index': 14, 'word': 'ne', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.49343053, 'index': 15, 'word': '##r', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.534215, 'index': 16, 'word': 'research', 'start': None, 'end': None}], [], [{'entity': 'B-NLP', 'score': 0.6420079, 'index': 5, 'word': 'ill', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.49805322, 'index': 6, 'word': '##ust', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.6968978, 'index': 7, 'word': '##rative', 'start': None, 'end': None}], [{'entity': 'B-NLP', 'score': 0.76632196, 'index': 1, 'word': 'translation', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.79842234, 'index': 2, 'word': 'models', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.85322833, 'index': 5, 'word': 'statistical', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.7696188, 'index': 6, 'word': 'machine', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.8486929, 'index': 7, 'word': 'translation', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.83225834, 'index': 11, 'word': 'parallel', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.8071234, 'index': 12, 'word': 'corp', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.76410216, 'index': 13, 'word': '##ora', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.38479316, 'index': 17, 'word': 'translated', 'start': None, 'end': None}], [{'entity': 'I-NLP', 'score': 0.33884597, 'index': 7, 'word': 'texts', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.4738604, 'index': 14, 'word': 'translation', 'start': None, 'end': None}], [{'entity': 'B-NLP', 'score': 0.803329, 'index': 4, 'word': 'translation', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.6382457, 'index': 5, 'word': 'studies', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.5906281, 'index': 11, 'word': 'translation', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.72824615, 'index': 17, 'word': 'translated', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.6533863, 'index': 18, 'word': 'language', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.81990534, 'index': 20, 'word': 'translation', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.40652815, 'index': 21, 'word': '##ese', 'start': None, 'end': None}], [{'entity': 'B-NLP', 'score': 0.7796371, 'index': 7, 'word': 'phrase', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.69943804, 'index': 8, 'word': 'tables', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.7141818, 'index': 11, 'word': 'parallel', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.69161844, 'index': 12, 'word': 'corp', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.76397544, 'index': 13, 'word': '##ora', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.8399483, 'index': 21, 'word': 'translation', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.77751285, 'index': 22, 'word': 'task', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.75052387, 'index': 29, 'word': 'corp', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.6186496, 'index': 30, 'word': '##ora', 'start': None, 'end': None}], [], [{'entity': 'B-NLP', 'score': 0.6959474, 'index': 11, 'word': 'translation', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.8322605, 'index': 14, 'word': 'phrase', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.850106, 'index': 15, 'word': 'tables', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.8566891, 'index': 19, 'word': 'translation', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.8251174, 'index': 20, 'word': 'model', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.863682, 'index': 26, 'word': 'translation', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.7034957, 'index': 27, 'word': '##ese', 'start': None, 'end': None}], [{'entity': 'B-NLP', 'score': 0.5969081, 'index': 4, 'word': 'adaptation', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.6994457, 'index': 12, 'word': 'mixture', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.7911635, 'index': 13, 'word': 'model', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.52331704, 'index': 16, 'word': '##pol', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.76780033, 'index': 18, 'word': 'phrase', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.8368294, 'index': 19, 'word': 'tables', 'start': None, 'end': None}], [{'entity': 'B-NLP', 'score': 0.6988138, 'index': 5, 'word': 'inter', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.8012769, 'index': 6, 'word': '##pol', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.44752887, 'index': 7, 'word': '##ation', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.55573964, 'index': 11, 'word': 'mini', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.8109537, 'index': 14, 'word': 'per', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.8084076, 'index': 15, 'word': '##plex', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.5352678, 'index': 16, 'word': '##ity', 'start': None, 'end': None}], [{'entity': 'B-NLP', 'score': 0.90196383, 'index': 5, 'word': 'entropy', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.88394576, 'index': 6, 'word': '-', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.81594044, 'index': 7, 'word': 'based', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.7986999, 'index': 8, 'word': 'measures', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.86391705, 'index': 14, 'word': 'target', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.759639, 'index': 15, 'word': '-', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.72395366, 'index': 16, 'word': 'language', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.7671269, 'index': 17, 'word': 'phrases', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.8579328, 'index': 19, 'word': 'translation', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.7234115, 'index': 20, 'word': '##ese', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.7068368, 'index': 27, 'word': 'ann', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.64791316, 'index': 28, 'word': '##ota', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.69269276, 'index': 31, 'word': 'parallel', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.7749054, 'index': 32, 'word': 'corpus', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.5077027, 'index': 40, 'word': 'translation', 'start': None, 'end': None}], [{'entity': 'B-NLP', 'score': 0.79729867, 'index': 11, 'word': 'phrase', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.79461986, 'index': 12, 'word': 'tables', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.82852954, 'index': 14, 'word': 'statistical', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.6986949, 'index': 15, 'word': 'machine', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.83802736, 'index': 16, 'word': 'translation', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.8512614, 'index': 17, 'word': 'systems', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.52281255, 'index': 22, 'word': 'statistical', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.4536245, 'index': 31, 'word': 'translation', 'start': None, 'end': None}], [{'entity': 'B-NLP', 'score': 0.5985416, 'index': 1, 'word': 'or', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.85979235, 'index': 2, 'word': '##th', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.8463978, 'index': 3, 'word': '##ographic', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.5131399, 'index': 6, 'word': 'languages', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.7208856, 'index': 12, 'word': 'un', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.8058385, 'index': 13, 'word': '##su', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.5513655, 'index': 14, 'word': '##per', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.6359475, 'index': 15, 'word': '##vis', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.46837932, 'index': 16, 'word': '##ed', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.5900572, 'index': 17, 'word': 'pro', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.4962844, 'index': 18, 'word': '##ba', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.49519077, 'index': 19, 'word': '##bilis', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.5638695, 'index': 20, 'word': '##tic', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.84802216, 'index': 21, 'word': 'trans', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.8602388, 'index': 22, 'word': '##duction', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.7905039, 'index': 24, 'word': 'dec', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.7450294, 'index': 25, 'word': '##ip', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.7388439, 'index': 26, 'word': '##her', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.5077258, 'index': 27, 'word': '##ment', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.7335475, 'index': 32, 'word': 'language', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.8816511, 'index': 33, 'word': 'pairs', 'start': None, 'end': None}], [{'entity': 'B-NLP', 'score': 0.84574354, 'index': 3, 'word': 'dec', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.83839333, 'index': 4, 'word': '##ip', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.8126833, 'index': 5, 'word': '##her', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.6606239, 'index': 6, 'word': '##ment', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.84125966, 'index': 7, 'word': 'models', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.74751776, 'index': 19, 'word': 'or', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.7501755, 'index': 20, 'word': '##th', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.7898646, 'index': 21, 'word': '##ographic', 'start': None, 'end': None}], [{'entity': 'B-NLP', 'score': 0.9043025, 'index': 4, 'word': 'log', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.78591585, 'index': 5, 'word': '-', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.71372354, 'index': 6, 'word': 'linear', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.8936966, 'index': 7, 'word': 'model', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.86769456, 'index': 9, 'word': 'late', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.83383757, 'index': 10, 'word': '##nt', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.7986254, 'index': 11, 'word': 'variables', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.8536596, 'index': 14, 'word': 'or', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.81113297, 'index': 15, 'word': '##th', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.878631, 'index': 16, 'word': '##ographic', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.7875119, 'index': 17, 'word': 'similarity', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.7820257, 'index': 18, 'word': 'features', 'start': None, 'end': None}], [{'entity': 'B-NLP', 'score': 0.78767174, 'index': 1, 'word': 'maximum', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.55115515, 'index': 2, 'word': 'likelihood', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.7652696, 'index': 3, 'word': 'training', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.87396216, 'index': 11, 'word': 'log', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.80863315, 'index': 12, 'word': '-', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.7316618, 'index': 13, 'word': 'linear', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.8993656, 'index': 14, 'word': 'model', 'start': None, 'end': None}], [{'entity': 'B-NLP', 'score': 0.8837607, 'index': 8, 'word': 'approximate', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.8009103, 'index': 9, 'word': 'inference', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.68348783, 'index': 11, 'word': 'marko', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.6450639, 'index': 12, 'word': '##v', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.8648187, 'index': 13, 'word': 'chain', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.8985816, 'index': 14, 'word': 'monte', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.89584124, 'index': 15, 'word': 'carlo', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.89677423, 'index': 16, 'word': 'sampling', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.88049346, 'index': 18, 'word': 'contrast', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.52459306, 'index': 19, 'word': '##ive', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.89916885, 'index': 20, 'word': 'diver', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.92205346, 'index': 21, 'word': '##gence', 'start': None, 'end': None}], [{'entity': 'B-NLP', 'score': 0.9129552, 'index': 7, 'word': 'log', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.8210592, 'index': 8, 'word': '-', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.7691511, 'index': 9, 'word': 'linear', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.8996983, 'index': 10, 'word': 'model', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.89720786, 'index': 12, 'word': 'contrast', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.69467324, 'index': 13, 'word': '##ive', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.8582047, 'index': 14, 'word': 'diver', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.9071415, 'index': 15, 'word': '##gence', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.8785727, 'index': 21, 'word': 'genera', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.62970567, 'index': 22, 'word': '##tive', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.8566291, 'index': 23, 'word': 'dec', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.6768516, 'index': 24, 'word': '##ip', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.7847698, 'index': 25, 'word': '##her', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.88667434, 'index': 26, 'word': '##ment', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.89228624, 'index': 27, 'word': 'models', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.8094408, 'index': 32, 'word': 'or', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.83960384, 'index': 33, 'word': '##th', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.8586152, 'index': 34, 'word': '##ographic', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.72999287, 'index': 35, 'word': 'features', 'start': None, 'end': None}], [{'entity': 'B-NLP', 'score': 0.7861486, 'index': 7, 'word': 'vo', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.59805137, 'index': 8, 'word': '##ca', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.5611433, 'index': 9, 'word': '##bular', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.6387388, 'index': 10, 'word': '##ies', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.5175675, 'index': 19, 'word': '-', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.5813304, 'index': 20, 'word': 'resource', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.6501839, 'index': 21, 'word': 'contexts', 'start': None, 'end': None}], [{'entity': 'B-NLP', 'score': 0.8550237, 'index': 11, 'word': 'data', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.89536923, 'index': 12, 'word': '-', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.8975524, 'index': 13, 'word': 'driven', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.6583681, 'index': 14, 'word': 'natural', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.8428605, 'index': 15, 'word': 'language', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.91286147, 'index': 16, 'word': 'generation', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.75698984, 'index': 25, 'word': 'fine', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.8063279, 'index': 26, 'word': '-', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.64809585, 'index': 27, 'word': 'grain', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.6661427, 'index': 28, 'word': '##ed', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.58231115, 'index': 29, 'word': 'interactions', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.72618264, 'index': 33, 'word': 'micro', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.48590547, 'index': 34, 'word': '##pl', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.48893985, 'index': 35, 'word': '##ann', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.6091248, 'index': 36, 'word': '##ing', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.5967668, 'index': 38, 'word': 'aggregation', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.82947576, 'index': 40, 'word': 'surface', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.8960101, 'index': 41, 'word': 'realization', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.8283928, 'index': 44, 'word': 'sentence', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.857729, 'index': 45, 'word': 'segment', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.9136349, 'index': 46, 'word': '##ation', 'start': None, 'end': None}], [{'entity': 'B-NLP', 'score': 0.54645646, 'index': 8, 'word': 'hybrid', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.5039321, 'index': 9, 'word': 'symbolic', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.75158954, 'index': 11, 'word': 'statistical', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.5890305, 'index': 12, 'word': 'approach', 'start': None, 'end': None}], [{'entity': 'B-NLP', 'score': 0.71339244, 'index': 7, 'word': 'hand', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.5987959, 'index': 8, 'word': '##written', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.8766141, 'index': 9, 'word': 'grammar', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.8916182, 'index': 12, 'word': 'statistical', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.8164083, 'index': 13, 'word': 'hyper', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.7315496, 'index': 14, 'word': '##tag', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.91539127, 'index': 15, 'word': '##ger', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.85316914, 'index': 19, 'word': 'surface', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.8168986, 'index': 20, 'word': 'realization', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.86031735, 'index': 21, 'word': 'algorithm', 'start': None, 'end': None}], [{'entity': 'B-NLP', 'score': 0.8255649, 'index': 6, 'word': 'verbal', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.5671641, 'index': 7, 'word': '##ization', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.8198849, 'index': 9, 'word': 'knowledge', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.60037154, 'index': 10, 'word': 'base', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.7887392, 'index': 11, 'word': 'que', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.7757705, 'index': 12, 'word': '##ries', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.7593781, 'index': 17, 'word': 'knowledge', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.8190116, 'index': 18, 'word': 'bases', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.8675035, 'index': 21, 'word': 'domain', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.694687, 'index': 22, 'word': 'independence', 'start': None, 'end': None}], [], [{'entity': 'B-NLP', 'score': 0.47866347, 'index': 2, 'word': 'quantitative', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.62973, 'index': 7, 'word': 'hybrid', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.6472015, 'index': 8, 'word': 'approach', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.76958984, 'index': 14, 'word': 'symbolic', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.5059275, 'index': 15, 'word': 'approach', 'start': None, 'end': None}], [{'entity': 'B-NLP', 'score': 0.5784687, 'index': 14, 'word': 'hybrid', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.6652094, 'index': 15, 'word': 'stat', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.7015226, 'index': 16, 'word': '##istic', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.6635854, 'index': 18, 'word': 'symbolic', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.915363, 'index': 19, 'word': 'system', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.86582285, 'index': 25, 'word': 'template', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.89928013, 'index': 26, 'word': '-', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.81974655, 'index': 27, 'word': 'based', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.8574495, 'index': 31, 'word': 'symbolic', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.88164586, 'index': 32, 'word': 'grammar', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.8373298, 'index': 33, 'word': 'based', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.5541847, 'index': 34, 'word': 'approach', 'start': None, 'end': None}], [{'entity': 'B-NLP', 'score': 0.5841876, 'index': 19, 'word': 'aggregation', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.8314902, 'index': 21, 'word': 'sentence', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.73477244, 'index': 22, 'word': 'segment', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.862866, 'index': 23, 'word': '##ation', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.8176436, 'index': 26, 'word': 'surface', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.85529023, 'index': 27, 'word': 'realization', 'start': None, 'end': None}], [{'entity': 'B-NLP', 'score': 0.90819556, 'index': 8, 'word': 'phenomenon', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.8959944, 'index': 9, 'word': '-', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.8359685, 'index': 10, 'word': 'oriented', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.8264331, 'index': 11, 'word': 'comparative', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.59163535, 'index': 12, 'word': 'analysis', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.85585105, 'index': 19, 'word': 'english', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.6191032, 'index': 20, 'word': 'resource', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.8771252, 'index': 21, 'word': 'semantic', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.8262246, 'index': 23, 'word': 'er', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.7706973, 'index': 24, 'word': '##s', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.8913255, 'index': 26, 'word': 'par', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.8924875, 'index': 27, 'word': '##sing', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.73894393, 'index': 29, 'word': 'classic', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.86326605, 'index': 31, 'word': 'knowledge', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.8586459, 'index': 32, 'word': '-', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.6785049, 'index': 33, 'word': 'intensive', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.8166441, 'index': 35, 'word': 'neural', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.49100366, 'index': 36, 'word': ',', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.85266536, 'index': 37, 'word': 'data', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.8837332, 'index': 38, 'word': '-', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.832385, 'index': 39, 'word': 'intensive', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.8262964, 'index': 40, 'word': 'models', 'start': None, 'end': None}], [{'entity': 'B-NLP', 'score': 0.73206013, 'index': 3, 'word': 'state', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.8598887, 'index': 4, 'word': '-', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.79743886, 'index': 5, 'word': 'of', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.83512795, 'index': 6, 'word': '-', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.7187188, 'index': 7, 'word': 'the', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.89002043, 'index': 8, 'word': '-', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.35433346, 'index': 9, 'word': 'art', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.730942, 'index': 10, 'word': 'neural', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.8017416, 'index': 11, 'word': 'nl', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.87000334, 'index': 12, 'word': '##p', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.51209426, 'index': 13, 'word': 'technologies', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.89033866, 'index': 16, 'word': 'factor', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.720368, 'index': 17, 'word': '##ization', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.91061056, 'index': 18, 'word': '-', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.8740676, 'index': 19, 'word': 'based', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.850778, 'index': 20, 'word': 'par', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.88367814, 'index': 21, 'word': '##ser', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.7756181, 'index': 27, 'word': 'elementary', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.6118563, 'index': 28, 'word': 'dependency', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.8931735, 'index': 29, 'word': 'structures', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.8295303, 'index': 35, 'word': 'data', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.92701674, 'index': 36, 'word': '-', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.89878213, 'index': 37, 'word': 'driven', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.90277636, 'index': 38, 'word': 'par', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.90033776, 'index': 39, 'word': '##ser', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.8171088, 'index': 40, 'word': '##s', 'start': None, 'end': None}], [{'entity': 'B-NLP', 'score': 0.8725776, 'index': 9, 'word': 'linguistic', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.7914938, 'index': 10, 'word': 'phenomena', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.8453355, 'index': 14, 'word': 'grammatical', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.77410346, 'index': 15, 'word': 'competence', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.7510201, 'index': 18, 'word': 'par', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.70597446, 'index': 19, 'word': '##ser', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.5825521, 'index': 20, 'word': '##s', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.79187196, 'index': 32, 'word': 'knowledge', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.7220739, 'index': 33, 'word': '-', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.8306852, 'index': 35, 'word': 'data', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.84157944, 'index': 36, 'word': '-', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.7091553, 'index': 37, 'word': 'intensive', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.7264356, 'index': 38, 'word': 'models', 'start': None, 'end': None}], [{'entity': 'B-NLP', 'score': 0.6584333, 'index': 12, 'word': 'representative', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.76025844, 'index': 13, 'word': 'par', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.64750093, 'index': 14, 'word': '##sing', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.77778053, 'index': 15, 'word': 'techniques', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.8111652, 'index': 22, 'word': 'par', 'start': None, 'end': None}, {'entity': 'B-NLP', 'score': 0.8096368, 'index': 23, 'word': '##ser', 'start': None, 'end': None}, {'entity': 'I-NLP', 'score': 0.6449356, 'index': 24, 'word': 'development', 'start': None, 'end': None}]]\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "pipe = pipeline(\"token-classification\", model=model, tokenizer=tokenizer)\n",
        "res =[]\n",
        "\n",
        "for s in df_test_sents['Sentence'].values:\n",
        "    print(\"SENT\",s)\n",
        "    res.append(pipe(s))\n",
        "\n",
        "\n",
        "print(res)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BzIXqafy8x0f",
        "outputId": "04c3a3de-a3d8-4c6d-c097-8321a103fc3e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "second , we define entropy - based measures that estimate the correspondence of target - language phrases to translationese , thereby eliminating the need to annotate the parallel corpus with information pertaining to the direction of translation .\n",
            "['O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'I-NLP', 'I-NLP', 'I-NLP', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'B-NLP', 'O', 'O', 'B-NLP', 'I-NLP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NLP', 'O']\n"
          ]
        }
      ],
      "source": [
        "sentence = \"Second , we define entropy - based measures that estimate the correspondence of target - language phrases to translationese , thereby eliminating the need to annotate the parallel corpus with information pertaining to the direction of translation .\"\n",
        "\n",
        "inputs = tokenizer(sentence, padding='max_length', truncation=True, max_length=MAX_LEN, return_tensors=\"pt\")\n",
        "\n",
        "# move to gpu\n",
        "ids = inputs[\"input_ids\"].to(device)\n",
        "mask = inputs[\"attention_mask\"].to(device)\n",
        "# forward pass\n",
        "outputs = model(ids, mask)\n",
        "logits = outputs[0]\n",
        "\n",
        "active_logits = logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n",
        "flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size*seq_len,) - predictions at the token level\n",
        "\n",
        "tokens = tokenizer.convert_ids_to_tokens(ids.squeeze().tolist())\n",
        "token_predictions = [id2label[i] for i in flattened_predictions.cpu().numpy()]\n",
        "wp_preds = list(zip(tokens, token_predictions)) # list of tuples. Each tuple = (wordpiece, prediction)\n",
        "\n",
        "word_level_predictions = []\n",
        "for pair in wp_preds:\n",
        "  if (pair[0].startswith(\" ##\")) or (pair[0] in ['[CLS]', '[SEP]', '[PAD]']):\n",
        "    # skip prediction\n",
        "    continue\n",
        "  else:\n",
        "    word_level_predictions.append(pair[1])\n",
        "\n",
        "# we join tokens, if they are not special ones\n",
        "str_rep = \" \".join([t[0] for t in wp_preds if t[0] not in ['[CLS]', '[SEP]', '[PAD]']]).replace(\" ##\", \"\")\n",
        "print(str_rep)\n",
        "print(word_level_predictions)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "# confusion matrix\n",
        "ref_labels=['B-NLP', 'I-NLP', 'O']\n",
        "conf_mat = confusion_matrix(predictions, labels, labels=ref_labels)\n",
        "sns.set(font_scale=1)\n",
        "x_axis_labels = ref_labels\n",
        "y_axis_labels = ref_labels\n",
        "matrix = sns.heatmap(conf_mat, annot=True, fmt='d', linewidths=.5, cmap='flare', xticklabels=x_axis_labels,\n",
        "                     yticklabels=y_axis_labels)\n",
        "matrix.set(xlabel='predicted', ylabel='actual')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
